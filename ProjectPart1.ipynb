{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba7624d",
   "metadata": {},
   "source": [
    "# Emotional Consistency among Political Ideologies: An Approach to Address Polarization on Youtube\n",
    "\n",
    "Group 5:\n",
    "- Chance Landis (ChancL), Hanna Lee (Lee10), Jason Sun (YongXs), Andy Wong (WongA22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502a114",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285d26",
   "metadata": {},
   "source": [
    "### Sources of Information\n",
    "- **AllSides**: A media bias tool that provides a rating based on \"multi-partisan Editorial Reviews by trained experts and Blind Bias Surveys™ in which participants rate content without knowing the source.\" We used this tool to determine how we should classify the most popular (based on subscriber count) YouTube channels we found. (Source: https://www.allsides.com/media-bias/media-bias-rating-methods)\n",
    "- **HypeAuitor**: A company that uses a data-driven approach to influencer marketing. In the process, they collated lists of YouTube based on category, subscriber count, and country. This allowed us to find YouTube channels that focused on news and politics with the most subscribers. (Source: https://hypeauditor.com/about/company/, https://hypeauditor.com/top-youtube-news-politics-united-states/)\n",
    "- **Pew Research Center**: A nonpartisan, nonprofit organization that conducts research on public opinion, demographic trends, and social issues. It provides data-driven insights into various aspects of social science issues, explicitly stating they do not take a stance on political issues. For our research, we relied on their studies on political ideologies and alignment with political parties as a reference. (Source: https://www.pewresearch.org/about/, https://www.pewresearch.org/politics/2016/06/22/5-views-of-parties-positions-on-issues-ideologies/)\n",
    "- **YouTube**: As a group, we've chosen to expand our collection of YouTube videos by selecting additional keywords associated with the ideology we're studying. Our focus will be on gathering comments from these videos to conduct our research.\n",
    "    - We used a combination of Andy and Hanna's code to get the comments from YouTube channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4129f",
   "metadata": {},
   "source": [
    "### Top 5 Democratic YouTube Channels\n",
    "Vice, Vox, MSNBC, The Daily Show, The Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.9/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7291af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cb379f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b54998d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "\n",
    "import googleapiclient\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c59b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call\n",
    "# vice: AIzaSyA2rNi_MI-3LQkBzzQ6Tn4EF0lgXWoilfc\n",
    "# vox: AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao\n",
    "# msnbc: AIzaSyBZDxP2HEW50EDfExcZJag7J2mRroZ9_vk\n",
    "# daily show: AIzaSyD8adQZlhLNVQrQXpU5-u3s1Y-9TZs20ik\n",
    "# young turk: AIzaSyB8yyrUrfQGLrlQRmF555oc1emrIDXF7yU\n",
    "API_KEY = \"AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdbfd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels\n",
    "channels = [\"Vice\", \"Vox\", \"msnbc\", \"thedailyshow\", \"TheYoungTurks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2784974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords\n",
    "keyword_lists = {\n",
    "    \"isis\": [\"ISIS\", \"Terrorism\", \"Radicalist\", \"Jihad\", \"Suicide Bombing\"],\n",
    "    \"guns\": [\"Gun\", \"Shooting\", \"School shooting\", \"Firearm\", \"Gun control\", \"NRA\", \"Second Amendment\"],\n",
    "    \"immigration\": [\"Immigration\", \"Border control\", \"Mexico\", \"Visa\", \"Citizenship\", \"Asylum\", \"Deportation\", \"Refugee\"],\n",
    "    \"economy\": [\"Economy\", \"Budget deficit\", \"Unemployed\", \"Inflation\", \"Interest rate\", \"Federal reserve\", \"Market\", \"Employment\"],\n",
    "    \"healthcare\": [\"Health care\", \"Medicaid\", \"Covid\", \"Obamacare\", \"Public health\", \"Insurance\"],\n",
    "    \"socioeco\": [\"Socio-economic\", \"Rich\", \"Poor\", \"Income inequality\", \"Poverty\", \"Wealth distribution\"],\n",
    "    \"abortion\": [\"Abortion\", \"Pregnancy\", \"Unwanted Pregnancy\", \"Roe\", \"Wade\", \"Pro-life\", \"Rape\", \"Incest\", \"Life of mother\", \"Religion\"],\n",
    "    \"climate\": [\"Climate change\", \"Global Warming\", \"Carbon\", \"Alternative Energy\", \"Climate\", \"Methane\", \"Emissions\", \"Gas\", \"Greenhouse\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5db44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting channel id based on name\n",
    "def get_channel_id(channel):  \n",
    "    channel_id = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        type=\"channel\",\n",
    "        q=channel\n",
    "    )\n",
    "\n",
    "    res_channel = channel_id.execute()\n",
    "    chan_id = res_channel[\"items\"][0][\"id\"][\"channelId\"]\n",
    "\n",
    "    return chan_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5094c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id using channel id\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel\n",
    "    )\n",
    "\n",
    "    res = request.execute()\n",
    "    uploads_playlist_id = res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "    return uploads_playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83d11a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vice\n",
      "Vox\n",
      "msnbc\n",
      "thedailyshow\n",
      "TheYoungTurks\n"
     ]
    }
   ],
   "source": [
    "up_id = []\n",
    "\n",
    "for channel in channels:\n",
    "    print(channel)\n",
    "    chan_id = get_channel_id(channel)\n",
    "    upload_id = get_upload_id(chan_id)\n",
    "    up_id.append(upload_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0439b99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UUn8zNIfYAQNdrFRrr8oibKw',\n",
       " 'UULXo7UDZvByw2ixzpQCufnA',\n",
       " 'UUaXkIU1QidjPwiAYu6GcHjg',\n",
       " 'UUwWhs_6x42TyRM4Wstoq8HA',\n",
       " 'UU1yBKRuGpC1tSM73A0ZjYjQ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed231e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Function to check if a video title contains any of the keywords\n",
    "def contains_keyword(title, keywords):\n",
    "    title_lower = title.lower()\n",
    "    words = word_tokenize(title_lower)\n",
    "    \n",
    "    # Stem each word in the title + keyword\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    for keyword in keywords:\n",
    "        keyword_stemmed = ps.stem(keyword.lower())\n",
    "        if keyword_stemmed in stemmed_words:\n",
    "            return keyword\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecfba1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch videos from a playlist and get title with keywords\n",
    "def keyword_videos(playlist_id, keywords, channel_name):\n",
    "    videos_info = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Make the next API request using the nextPageToken\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=next_page_token\n",
    "        ) \n",
    "        res = request.execute()\n",
    "\n",
    "        # Process the response and save video info\n",
    "        for v in res[\"items\"]:\n",
    "            video_title = v[\"snippet\"][\"title\"]\n",
    "            detected_word = contains_keyword(video_title, keywords)\n",
    "            if detected_word:\n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                video_views = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "                # Append video information with views to videos_info list\n",
    "                videos_info.append({\n",
    "                    \"id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                    \"title\": video_title,\n",
    "                    \"keyword\": detected_word,\n",
    "                    \"published_at\": v[\"snippet\"][\"publishedAt\"],\n",
    "                    \"VideoViews\": video_views\n",
    "                })\n",
    "        # Update the nextPageToken for the next iteration\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token or (len(videos_info) > 40):\n",
    "            break\n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45003e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for channel, upload_id in zip(channels, up_id):\n",
    "#     for keyword_name, keywords in keyword_lists.items():\n",
    "#         videos_info = keyword_videos('UUn8zNIfYAQNdrFRrr8oibKw', keywords, 'Vice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc045344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(channels, up_id, keyword_lists, limit=30):\n",
    "    # Function to fetch videos from a playlist and get title with keywordsand \n",
    "    def keyword_videos(playlist_id, keywords, channel_name):\n",
    "        videos_info = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            # Make the next API request using the nextPageToken\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                pageToken=next_page_token\n",
    "            ) \n",
    "            res = request.execute()\n",
    "\n",
    "            # Process the response and save video info\n",
    "            for v in res[\"items\"]:\n",
    "                video_title = v[\"snippet\"][\"title\"]\n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    videos_info.append(\n",
    "                    {\n",
    "                        \"channel\": channel_name,\n",
    "                        \"video_id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"]\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "            # Update the nextPageToken for the next iteration\n",
    "            next_page_token = res.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token or (len(videos_info) > 40):\n",
    "                break\n",
    "        return videos_info\n",
    "\n",
    "    # Function for getting top 30 relevant comments for a list of videos\n",
    "    def get_vid_comments(vid_lst, limit):\n",
    "        vids_final = []\n",
    "\n",
    "        # Iterate through each video in the video list\n",
    "        for vid in vid_lst:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    videoId=vid['video_id'],\n",
    "                    part='id,snippet,replies',\n",
    "                    textFormat='plainText',\n",
    "                    order='relevance',\n",
    "                    maxResults=50)\n",
    "                res = request.execute()\n",
    "\n",
    "                # Iterate through each comment\n",
    "                for v in res[\"items\"]:\n",
    "\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = vid.copy()\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "\n",
    "                nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                while nextPageToken:\n",
    "                    try:\n",
    "                        request = youtube.commentThreads().list(\n",
    "                            videoId=vid['video_id'],\n",
    "                            part='id,snippet,replies',\n",
    "                            textFormat='plainText',\n",
    "                            order='relevance',\n",
    "                            maxResults=50,\n",
    "                            pageToken=nextPageToken)\n",
    "\n",
    "                        res = request.execute()\n",
    "\n",
    "                        nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                        for v in res[\"items\"]:\n",
    "                            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                            vid_temp = vid.copy()\n",
    "                            vid_temp.update({'CommentId':v['id']})\n",
    "                            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                            vids_final.append(vid_temp)\n",
    "\n",
    "                        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                        if len(vids_final) >= limit:\n",
    "                            return vids_final\n",
    "                    except KeyError:\n",
    "                        break\n",
    "\n",
    "            # Error handling for videos with disabled comments\n",
    "            # Got the answer format from StackOverflow (https://stackoverflow.com/questions/19342111/get-http-error-code-from-requests-exceptions-httperror)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments are disabled for the video with videoId: {vid['video_id']}\")\n",
    "                else:\n",
    "                    print(\"An HTTP error occurred:\", e)\n",
    "                # Continue to the next video\n",
    "                continue\n",
    "                \n",
    "        return vids_final\n",
    "    \n",
    "    all_comments = []\n",
    "    #for channel, upload_id in zip(channels, up_id):\n",
    "    for keyword_name, keywords in keyword_lists.items():\n",
    "        videos_info = keyword_videos('UUaXkIU1QidjPwiAYu6GcHjg', keywords, 'MSNBC')\n",
    "        video_comments = get_vid_comments(videos_info, limit)\n",
    "        all_comments.extend(video_comments)\n",
    "    \n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a0664",
   "metadata": {},
   "source": [
    "#### Vice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50afc5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments are disabled for the video with videoId: EEIvWNhuL8U\n"
     ]
    }
   ],
   "source": [
    "vice_comments = get_video_comments('Vice', 'UUn8zNIfYAQNdrFRrr8oibKw', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f04e1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "742ce035",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugws1dFQrp7AovnexrB4AaABAg',\n",
       "  'CommentTitle': 'Bless the hard work of journalists! Seeing the deplorable and terrible things done by monstrous groups like ISIS in one spot must be so difficult. We’re with you!',\n",
       "  'CommentCreationTime': '2022-04-12T22:53:43Z',\n",
       "  'CommentLikes': 146},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxcNLZW2rAeMBklWD14AaABAg',\n",
       "  'CommentTitle': \"Also I can't imagine  the amount mental trauma this work puts these journalists and their teams  undergo having to file through hours of footage of some of the most horrific acts enacted upon people in order to try and piece together what really happened.  If they can uncover even some of truth that is a very big step forward to those victims who are still alive and hopefullt it can highlight those respsonsible\",\n",
       "  'CommentCreationTime': '2022-04-11T16:49:08Z',\n",
       "  'CommentLikes': 726},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzFcqbEHILJ93hjvqh4AaABAg',\n",
       "  'CommentTitle': 'This is so heartbreaking. What a horrific display of violence. I’m so sorry this happened to these people. I’m praying for their families 🥺🙏🏽',\n",
       "  'CommentCreationTime': '2022-04-11T15:08:45Z',\n",
       "  'CommentLikes': 251},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxYrGf2RJZeKbFEap14AaABAg',\n",
       "  'CommentTitle': \"7:35 Social-media shouldn't be just summarily deleting flagged content when they remove stuff, they should be quarantining it, forwarding it to law-enforcement and making it available for investigators to requisition. We know they never delete any data anyway, so it's not like the media is actually _gone_ so it SHOULD still be available for the relevant authorities. Problem solved.\",\n",
       "  'CommentCreationTime': '2022-04-11T15:59:18Z',\n",
       "  'CommentLikes': 219},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugx5rmcME6ua2pMkojt4AaABAg',\n",
       "  'CommentTitle': 'VICE NEVER Dissapoints! Amazing documentaries! Thank you 🙏🏽 for your amazing humanity rights findings not only for one race or culture but for everyone god bless!! 🙌',\n",
       "  'CommentCreationTime': '2022-04-11T15:04:32Z',\n",
       "  'CommentLikes': 127},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzzbQ-K80UX_H02FPR4AaABAg',\n",
       "  'CommentTitle': 'Surely if social media companies pull such content they need to forward onto investigators as part of their due diligence in terms of business-human rights.',\n",
       "  'CommentCreationTime': '2022-04-11T15:14:45Z',\n",
       "  'CommentLikes': 161},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgysAY65Ywm8Z54rrMZ4AaABAg',\n",
       "  'CommentTitle': 'Wow, this was interesting and I learned some things I didn’t know before. Thanks for “shedding light so others can see”, hey I like that! I appreciate good, honest journalism. Props. 👊',\n",
       "  'CommentCreationTime': '2022-04-11T18:23:50Z',\n",
       "  'CommentLikes': 33},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugwk0pK1S0kCyDg-QoZ4AaABAg',\n",
       "  'CommentTitle': 'Another great video by VICE, sometimes u guys produce mediocre content, but a lot of great stuff... Been following since the older documentary days about north korea',\n",
       "  'CommentCreationTime': '2022-04-12T05:37:50Z',\n",
       "  'CommentLikes': 93},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz8RjIShFG5SQ6XvyZ4AaABAg',\n",
       "  'CommentTitle': 'You guys are literally next level heros!!',\n",
       "  'CommentCreationTime': '2022-04-17T01:07:35Z',\n",
       "  'CommentLikes': 5},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz1DoNZkAejBAok2Xd4AaABAg',\n",
       "  'CommentTitle': 'You guys r the most realistic, journalism i have seen. \\nThank you so much for bringing this',\n",
       "  'CommentCreationTime': '2022-04-11T16:59:17Z',\n",
       "  'CommentLikes': 4}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vice_comments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec228a9d",
   "metadata": {},
   "source": [
    "### Vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e72471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_comments = get_video_comments('Vox', 'UULXo7UDZvByw2ixzpQCufnA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fd6dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vox_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4fbd51d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxCgDXiTTfjLBdkAE94AaABAg',\n",
       "  'CommentTitle': '\"It was a low degree of terror.\"\\nThis man could barely choke out those words a half century later...\\nLow degree?  I don\\'t think so.',\n",
       "  'CommentCreationTime': '2019-11-05T08:38:22Z',\n",
       "  'CommentLikes': 13004},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzpaYXLMtGz3ymErOJ4AaABAg',\n",
       "  'CommentTitle': 'Could you imagine having to question if every single person you engage with is a plant by some organization?',\n",
       "  'CommentCreationTime': '2019-11-04T13:16:13Z',\n",
       "  'CommentLikes': 7158},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgweQtEREv31yKgtumB4AaABAg',\n",
       "  'CommentTitle': 'That poor man never trusted another person and I feel so bad for him. I tear up every time he does. You can still see how hard he thinks before he speaks.',\n",
       "  'CommentCreationTime': '2019-11-30T14:30:31Z',\n",
       "  'CommentLikes': 4516},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwaArmK_X6VOrDrjrZ4AaABAg',\n",
       "  'CommentTitle': 'And this is only 1 person’s story.',\n",
       "  'CommentCreationTime': '2019-11-06T13:46:52Z',\n",
       "  'CommentLikes': 2228},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxzPYwa0EGXhdYhtMF4AaABAg',\n",
       "  'CommentTitle': '\"These interrogators, the investigators, they weren\\'t very bright people, but they had tremendous power over others. Over us.\" Some things absolutely never change',\n",
       "  'CommentCreationTime': '2019-11-05T02:56:20Z',\n",
       "  'CommentLikes': 3026},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxL6re97jEJbZIqxBZ4AaABAg',\n",
       "  'CommentTitle': 'Boomer: it was so much better when I was younger',\n",
       "  'CommentCreationTime': '2019-11-05T17:15:07Z',\n",
       "  'CommentLikes': 3903},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'Ugwv4GDXYpk7qiHTInZ4AaABAg',\n",
       "  'CommentTitle': '\"They weren\\'t very bright people but they have tremendous power over others, over us.\" Sound familiar?',\n",
       "  'CommentCreationTime': '2019-11-04T23:31:59Z',\n",
       "  'CommentLikes': 3657},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzDydG5LiAj1if6GvR4AaABAg',\n",
       "  'CommentTitle': 'I just want to offer a heartfelt thanks to the gentleman who was interviewed for this video. Sir, your courage and willingness to share about what was obviously a terrible time in your life (one that apparently affects you to this day) is invaluable to those of us who might otherwise have trouble connecting with that time and those events.',\n",
       "  'CommentCreationTime': '2019-11-04T14:09:00Z',\n",
       "  'CommentLikes': 11509},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwwabWjgasZiMUny_p4AaABAg',\n",
       "  'CommentTitle': 'I want to give that man a hug after what he went through',\n",
       "  'CommentCreationTime': '2019-11-04T14:27:25Z',\n",
       "  'CommentLikes': 3315},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgykYfLlDY9Gxiax96d4AaABAg',\n",
       "  'CommentTitle': 'Imagine hating someone because of their sexuality that has absolutely nothing to do with you.. lol',\n",
       "  'CommentCreationTime': '2020-07-10T15:23:49Z',\n",
       "  'CommentLikes': 2679}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vox_comments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd5d4e",
   "metadata": {},
   "source": [
    "#### MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10d9a8a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAafVBUOkNNV0pBU0lRUTBRMk1VUXpRalJETVVWR1JqVkNOaWdCU05MNXp1eU51SVFEVUFGYU5pSkRhR2hXVmxkR1dXRXdiRlpOVmtad1drZHdVV1F5YkVKWFdGVXlVakpPU1dGdFkxTkRkMm94ZWpnMmRVSm9SRkZvVFVWNEln&key=AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m msnbc_comments \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSNBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUaXkIU1QidjPwiAYu6GcHjg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mget_video_comments\u001b[0;34m(channels, up_id, keyword_lists, limit)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#for channel, upload_id in zip(channels, up_id):\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword_name, keywords \u001b[38;5;129;01min\u001b[39;00m keyword_lists\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 110\u001b[0m     videos_info \u001b[38;5;241m=\u001b[39m \u001b[43mkeyword_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUaXkIU1QidjPwiAYu6GcHjg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSNBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     video_comments \u001b[38;5;241m=\u001b[39m get_vid_comments(videos_info, limit)\n\u001b[1;32m    112\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(video_comments)\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mget_video_comments.<locals>.keyword_videos\u001b[0;34m(playlist_id, keywords, channel_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Make the next API request using the nextPageToken\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mplaylistItems()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     10\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         playlistId\u001b[38;5;241m=\u001b[39mplaylist_id,\n\u001b[1;32m     12\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[1;32m     13\u001b[0m     ) \n\u001b[0;32m---> 14\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Process the response and save video info\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAafVBUOkNNV0pBU0lRUTBRMk1VUXpRalJETVVWR1JqVkNOaWdCU05MNXp1eU51SVFEVUFGYU5pSkRhR2hXVmxkR1dXRXdiRlpOVmtad1drZHdVV1F5YkVKWFdGVXlVakpPU1dGdFkxTkRkMm94ZWpnMmRVSm9SRkZvVFVWNEln&key=AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "msnbc_comments = get_video_comments('MSNBC', 'UUaXkIU1QidjPwiAYu6GcHjg', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msnbc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_comments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711fc85",
   "metadata": {},
   "source": [
    "#### The Daily Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a89c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyshow_comments = get_video_comments('The Daily Show', 'UUwWhs_6x42TyRM4Wstoq8HA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dailyshow_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68cec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyshow_comments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40f656",
   "metadata": {},
   "source": [
    "#### Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc32cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments = get_video_comments('The Young Turks', 'UU1yBKRuGpC1tSM73A0ZjYjQ', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2161f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yturk_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287e782",
   "metadata": {},
   "source": [
    "## Andy's Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e60e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id of a channel\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(part='contentDetails', forUsername=channel)\n",
    "    res = request.execute()\n",
    "    return res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "# Function for retrieving all vids within the upload playlist of a channel, stopping once a limit INT has been reached\n",
    "def get_vids(channel, limit, keywords, ideology):\n",
    "    \n",
    "    # Output list\n",
    "    vid_lst=[]\n",
    "\n",
    "    request = youtube.playlistItems().list(part='snippet',playlistId=get_upload_id(channel),maxResults=50)\n",
    "        \n",
    "    res = request.execute()\n",
    "    nextPageToken = res['nextPageToken']\n",
    "\n",
    "    # Iterate through each video in the playlist\n",
    "    for v in res[\"items\"]:\n",
    "\n",
    "        # Normalization of video title to check for keywords\n",
    "        title = v['snippet']['title']\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "        # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "        counter = 0\n",
    "        for word in title.split():\n",
    "            counter = 0\n",
    "            if word in keywords:\n",
    "                counter += 1\n",
    "        if counter == 0:\n",
    "            continue\n",
    "\n",
    "        # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "        vid_dict = {}\n",
    "        vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "        vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "        vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "        vid_dict['Ideology'] = ideology\n",
    "\n",
    "        # Separate Resource Call to retrieve video views\n",
    "        views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "        view_temp = views.execute()\n",
    "        vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "        # Append dictionary to greater list\n",
    "        vid_lst.append(vid_dict)\n",
    "\n",
    "    # Iterate until no more next page\n",
    "    while nextPageToken:\n",
    "        try:\n",
    "            request = youtube.playlistItems().list(part='snippet', playlistId=get_upload_id(channel), maxResults=50, pageToken = res['nextPageToken'])                \n",
    "            res = request.execute()\n",
    "\n",
    "            # Redefine next page token to check @ next iteration\n",
    "            nextPageToken = res['nextPageToken']\n",
    "\n",
    "            # Iterate through each video\n",
    "            for v in res[\"items\"]:\n",
    "\n",
    "                # Normalization of video title to check for keywords\n",
    "                title = v['snippet']['title']\n",
    "                title = title.lower()\n",
    "                title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "                # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "                counter = 0\n",
    "                for word in title.split():\n",
    "                    if word in keywords:\n",
    "                        counter += 1\n",
    "                if counter == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "                vid_dict = {}\n",
    "                vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "                vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "                vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "                                \n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "                \n",
    "                vid_lst.append(vid_dict)\n",
    "\n",
    "            # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "            if len(vid_lst) >= limit:\n",
    "                return(vid_lst)\n",
    "\n",
    "        # Error case handling\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "# Function for getting top 30 relevant comments for a list of videos\n",
    "def get_vid_comments(vid_lst, limit):\n",
    "    vids_final = []\n",
    "\n",
    "    # Iterate through each video in the video list\n",
    "    for vid in vid_lst:\n",
    "        \n",
    "        request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "        res = request.execute()\n",
    "\n",
    "        # Iterate through each comment\n",
    "        for v in res[\"items\"]:\n",
    "            \n",
    "            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "            vid_temp = copy.copy(vid)\n",
    "            vid_temp.update({'CommentId':v['id']})\n",
    "            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "            vids_final.append(vid_temp)\n",
    "\n",
    "        while nextPageToken:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "                res = request.execute()\n",
    "        \n",
    "                nextPageToken = res['nextPageToken']\n",
    "                \n",
    "                for v in res[\"items\"]:\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = copy.copy(vid)\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "                    \n",
    "                # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                if len(vids_final) >= limit:\n",
    "                    return(vids_final)\n",
    "            except KeyError:\n",
    "                break\n",
    "            \n",
    "    return vids_final\n",
    "\n",
    "# from Lab9\n",
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "        #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')\n",
    "\n",
    "def df_clean_process(df):\n",
    "\n",
    "    # Change datetime to date\n",
    "    df['VideoPublishedDate'] = df['VideoPublishedDate'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "    df['CommentCreationTime'] = df['CommentCreationTime'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "\n",
    "    # Check NaN, if < 10% of total dataset, drop NaN\n",
    "    if df.isnull().values.any():\n",
    "        if len(df[df.isna().any(axis=1)]) < len(df) * 0.1:\n",
    "            df = df.dropna()\n",
    "\n",
    "    # Split into separate df for computational load reduction\n",
    "    title_df = df[['ChannelName', 'VideoTitle', 'VideoPublishedDate', 'VideoViews', 'Ideology']].drop_duplicates()\n",
    "    comment_df = df[['ChannelName', 'VideoViews', 'CommentTitle', 'CommentCreationTime', 'CommentLikes', 'Ideology']]\n",
    "\n",
    "    # tokenize\n",
    "    title_df['TweetToken'] = title_df['VideoTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "    comment_df['TweetToken'] = comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "\n",
    "    # clean\n",
    "    title_df['Cleaned'] = title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "    comment_df['Cleaned'] = comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "\n",
    "    return (title_df, comment_df)\n",
    "\n",
    "    # Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define channels\n",
    "channels_left = ['VICE', 'Vox', 'MSNBC', 'The Daily Show', 'TheYoungTurks']\n",
    "channels_right = ['Fox News', 'Ben Shapiro', 'StevenCrowder', 'Daily Mail', 'DailyWire+']\n",
    "\n",
    "# define key ideologies/associated keywords to look for in title\n",
    "isis_keywords = ['terrorism', 'terrorist', 'extremism', 'radicalist', 'radicalism']\n",
    "guns_keywords = ['shooting', 'shootings', 'school shooting', 'school shootings', 'firearms', 'firearm', 'gun', 'gun control', 'guns', 'nra', 'second amendment']\n",
    "immigration_keywords = ['border control', 'mexico', 'visa', 'citizenship', 'asylum', 'deportation', 'refugee']\n",
    "economy_keywords = ['budget', 'budget deficit', 'unemployed', 'inflation', 'interest rate',' federal reserve', 'market', 'employment']\n",
    "health_care_keywords = ['medicaid', 'covid', 'obamacare', 'public health', 'insurance']\n",
    "socioeconomic_keywords = ['rich', 'poor', 'income inequality', 'poverty',' wealth distribution']\n",
    "abortion_keywords = ['pregnancy', 'unwanted pregnancy', 'roe', 'wade', 'abortion', 'pro-life', 'rape', 'incest', 'life of mother', 'religion']\n",
    "climate_change_keywords = ['global warming', 'carbon', 'alternative energy', 'climate', 'methane', 'emissions','gas','greenhouse']\n",
    "\n",
    "# Define for iteration\n",
    "keywords = [isis_keywords, guns_keywords, immigration_keywords, economy_keywords, health_care_keywords, socioeconomic_keywords, abortion_keywords, climate_change_keywords]\n",
    "\n",
    "# Pre-define empty df\n",
    "left_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "\n",
    "# Loop through all left channels\n",
    "for channel in channels_left:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        left_df = pd.concat([left_df,temp_df])\n",
    "\n",
    "# Pre-define empty df\n",
    "right_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)\n",
    "# Loop through all right channels\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
