{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotional Consistency among Political Ideologies: An Approach to Address Polarization on Youtube\n",
    "\n",
    "Group 5:\n",
    "- Chance Landis (ChancL), Hanna Lee (Lee10), Jason Sun (YongXs), Andy Wong (WongA22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Work\n",
    "- Hanna: Democratic-leaning Channels Data Collection\n",
    "- Chance: Republican-leaning Channels Data Collection\n",
    "- Andy: Exploratory Data Analysis\n",
    "- Jason: Text Documentation\n",
    "\n",
    "## Problem Statement\n",
    "We want to figure out whether the current political polarization is associated with the emotional values expressed by each party. Due to the current politically charged environment of our country, the semblance of sympathizing toward a value that is not related to your political party causes backlash. This fear of backlash can create a “false” polarized environment, which is an extension of the fear itself. The question that arises is whether these boundaries are reinforced by the people themselves and/or external factors, like social media.\n",
    "\n",
    "## Research Question\n",
    "- Do political parties exhibit similar emotional responses to differing ideologies?\n",
    "\n",
    "## Data Collection\n",
    "To investigate this topic, we will analyze content from the top five YouTube channels associated with Democratic and Republican viewpoints, based on subscriber counts. The channels selected for the study are:\n",
    "- **Democratic-leaning Channels**: Vice, Vox, MSNBC, The Daily Show, The Young Turks\n",
    "- **Republican-leaning Channels**: Fox News, Ben Shapiro, Steven Crowder, The Daily Mail, The Daily Wire\n",
    "\n",
    "We have identified eight key ideologies for this analysis to understand if there are emotional differences in how political parties discuss these topics. For each ideology, a set of keywords has been established to facilitate data scraping:\n",
    "\n",
    "- **ISIS**: Terrorism, Extremism, Radical\n",
    "- **Guns**: Shootings, School shooting, Firearms, Gun control, NRA, Second Amendment\n",
    "- **Immigration**: Border control, Mexico, Visa /Citizenship, Asylum, Deportation, Refugee\n",
    "- **Economy**: Budget deficit, Unemployment, Inflation, Interest rate, Federal Reserve, Market, Employment\n",
    "- **Health care**: Medicaid, Covid, Obamacare, Public health, Insurance\n",
    "- **Socio-economic**: Rich / poor, Income inequality, Poverty, Wealth distribution\n",
    "- **Abortion**: Pregnancy, Unwanted Pregnancy, Roe, Wade, Abortion, Pro-life, Rape, Incest, Life of mother, Religion\n",
    "- **Climate change**: Global Warming, Carbo, Alternative Energy, Climate, Methane, Emissions, Gas, Greenhouse\n",
    "\n",
    "### Data Collection Pain Points\n",
    "The amount of data and the data collection method created significant challenges due to the API call exceeding the alloted amount. As a result, we developed 2 different approaches to data collection. First is a piecemeal approach. That means we went channel by channel then collected the videos and comments. This method was partially successful. Due to the method of the data call, we could not get as many comments as we would have preferred. Also, we ran out of API creations before we could get the final channel, \"The Daily Show.\"\n",
    "\n",
    "The second method was extracting all the video ids then separately calling for the comments based on the video ids. This method was also partially successfull as the API calls timed out before collecting the video ids for all the channels. \n",
    "\n",
    "We are still in the process of doing a cohesive data collection, but we do need to wait so we can get more APIs and get guidance on how to efficient gather all necessary information.\n",
    "\n",
    "## Sources of Information\n",
    "- **AllSides**: A media bias tool that provides a rating based on \"multi-partisan Editorial Reviews by trained experts and Blind Bias Surveys™ in which participants rate content without knowing the source.\" We used this tool to determine how we should classify the most popular (based on subscriber count) YouTube channels we found. (Source: https://www.allsides.com/media-bias/media-bias-rating-methods)\n",
    "- **HypeAuitor**: A company that uses a data-driven approach to influencer marketing. In the process, they collated lists of YouTube based on category, subscriber count, and country. This allowed us to find YouTube channels that focused on news and politics with the most subscribers. (Source: https://hypeauditor.com/about/company/, https://hypeauditor.com/top-youtube-news-politics-united-states/)\n",
    "- **Pew Research Center**: A nonpartisan, nonprofit organization that conducts research on public opinion, demographic trends, and social issues. It provides data-driven insights into various aspects of social science issues, explicitly stating they do not take a stance on political issues. For our research, we relied on their studies on political ideologies and alignment with political parties as a reference. (Source: https://www.pewresearch.org/about/, https://www.pewresearch.org/politics/2016/06/22/5-views-of-parties-positions-on-issues-ideologies/)\n",
    "- **YouTube**: As a group, we've chosen to expand our collection of YouTube videos by selecting additional keywords associated with the ideology we're studying. Our focus will be on gathering comments from these videos to conduct our research.\n",
    "    - We used a combination of Andy and Hanna's code to get the comments from YouTube channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install nrclex --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# imports\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import googleapiclient\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import casual\n",
    "\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lists for Required Info and Function for Necessary Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call\n",
    "API_KEY = \"AIzaSyD9fIFqDX7zzn8RP3mj1typ9zXxJECtujg\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels for Democractic YouTube channels\n",
    "channels = [\"Vice\", \"Vox\", \"msnbc\", \"thedailyshow\", \"TheYoungTurks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels for Republican YouTube channels\n",
    "channels_right = [\"BenShapiro\", \"StevenCrowder\", \"FoxNews\", \"DailyWirePlus\", \"dailymail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish keyword dictionary\n",
    "keyword_lists = {\n",
    "    \"isis\": [\"ISIS\", \"Terrorism\", \"Extremism\", \"Radicalist\"],\n",
    "    \"guns\": [\"Gun\", \"Shooting\", \"School shooting\", \"Firearm\", \"Gun control\", \"NRA\", \"Second Amendment\"],\n",
    "    \"immigration\": [\"Immigration\", \"Border control\", \"Mexico\", \"Visa\", \"Citizenship\", \"Asylum\", \"Deportation\", \"Refugee\"],\n",
    "    \"economy\": [\"Economy\", \"Budget deficit\", \"Unemployed\", \"Inflation\", \"Interest rate\", \"Federal reserve\", \"Market\", \"Employment\"],\n",
    "    \"healthcare\": [\"Health care\", \"Medicaid\", \"Covid\", \"Obamacare\", \"Public health\", \"Insurance\"],\n",
    "    \"socioeco\": [\"Socio-economic\", \"Rich\", \"Poor\", \"Income inequality\", \"Poverty\", \"Wealth distribution\"],\n",
    "    \"abortion\": [\"Abortion\", \"Pregnancy\", \"Unwanted Pregnancy\", \"Roe\", \"Wade\", \"Pro-life\", \"Rape\", \"Incest\", \"Life of mother\", \"Religion\"],\n",
    "    \"climate\": [\"Climate change\", \"Global Warming\", \"Carbon\", \"Alternative Energy\", \"Climate\", \"Methane\", \"Emissions\", \"Gas\", \"Greenhouse\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting channel id based on name\n",
    "def get_channel_id(channel):  \n",
    "    channel_id = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        type=\"channel\",\n",
    "        q=channel\n",
    "    )\n",
    "\n",
    "    res_channel = channel_id.execute()\n",
    "    chan_id = res_channel[\"items\"][0][\"id\"][\"channelId\"]\n",
    "\n",
    "    return chan_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id using channel id\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel\n",
    "    )\n",
    "\n",
    "    res = request.execute()\n",
    "    uploads_playlist_id = res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "    return uploads_playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Function to check if a video title contains any of the keywords\n",
    "def contains_keyword(title, keywords):\n",
    "    title_lower = title.lower()\n",
    "    words = word_tokenize(title_lower)\n",
    "    \n",
    "    # Stem each word in the title + keyword\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    for keyword in keywords:\n",
    "        keyword_stemmed = ps.stem(keyword.lower())\n",
    "        if keyword_stemmed in stemmed_words:\n",
    "            return keyword\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 different functions were used to gather comments. For Republican YouTube Channels, \"keyword_videos_right\" was used and then iterated on to gather comments. For the Democratic YouTube Channels, \"get_video_comments\" was used which is a compilation of multiple functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch videos from a playlist and get title with keywords\n",
    "def keyword_videos_right(playlist_id, channel_name, dict_list):\n",
    "    videos_info = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Make the next API request using the nextPageToken\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=next_page_token\n",
    "        ) \n",
    "        res = request.execute()\n",
    "\n",
    "        # Process the response and save video info\n",
    "        for v in res[\"items\"]:\n",
    "            video_title = v[\"snippet\"][\"title\"]\n",
    "            for keyword_name, keywords in keyword_lists.items():\n",
    "            \n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    # Separate Resource Call to retrieve video views\n",
    "                    views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                    view_temp = views.execute()\n",
    "                    video_views = view_temp['items'][0]['statistics']['viewCount']\n",
    "    \n",
    "                    # Append video information with views to videos_info list\n",
    "                    dict_list.append({\n",
    "                        \"id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"channel_name\" : v['snippet']['channelTitle'],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"],\n",
    "                        \"VideoViews\": video_views\n",
    "                    })\n",
    "        # Update the nextPageToken for the next iteration\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token or (len(videos_info) > 60):\n",
    "            break\n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get comments\n",
    "def get_video_comments(channels, up_id, keyword_lists, limit=30):\n",
    "    # Function to fetch videos from a playlist and get title with keywordsand \n",
    "    def keyword_videos(playlist_id, keywords, channel_name):\n",
    "        videos_info = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            # Make the next API request using the nextPageToken\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                pageToken=next_page_token\n",
    "            ) \n",
    "            res = request.execute()\n",
    "\n",
    "            # Process the response and save video info\n",
    "            for v in res[\"items\"]:\n",
    "                video_title = v[\"snippet\"][\"title\"]\n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    videos_info.append(\n",
    "                    {\n",
    "                        \"channel\": channel_name,\n",
    "                        \"video_id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"]\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "            # Update the nextPageToken for the next iteration\n",
    "            next_page_token = res.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token or (len(videos_info) > 15):\n",
    "                break\n",
    "        return videos_info\n",
    "\n",
    "    # Function for getting top 30 relevant comments for a list of videos\n",
    "    def get_vid_comments(vid_lst, limit):\n",
    "        vids_final = []\n",
    "\n",
    "        # Iterate through each video in the video list\n",
    "        for vid in vid_lst:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    videoId=vid['video_id'],\n",
    "                    part='id,snippet,replies',\n",
    "                    textFormat='plainText',\n",
    "                    order='relevance',\n",
    "                    maxResults=50)\n",
    "                res = request.execute()\n",
    "\n",
    "                # Iterate through each comment\n",
    "                for v in res[\"items\"]:\n",
    "\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = vid.copy()\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "\n",
    "                nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                while nextPageToken:\n",
    "                    try:\n",
    "                        request = youtube.commentThreads().list(\n",
    "                            videoId=vid['video_id'],\n",
    "                            part='id,snippet,replies',\n",
    "                            textFormat='plainText',\n",
    "                            order='relevance',\n",
    "                            maxResults=50,\n",
    "                            pageToken=nextPageToken)\n",
    "\n",
    "                        res = request.execute()\n",
    "\n",
    "                        nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                        for v in res[\"items\"]:\n",
    "                            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                            vid_temp = vid.copy()\n",
    "                            vid_temp.update({'CommentId':v['id']})\n",
    "                            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                            vids_final.append(vid_temp)\n",
    "\n",
    "                        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                        if len(vids_final) >= limit:\n",
    "                            return vids_final\n",
    "                    except KeyError:\n",
    "                        break\n",
    "\n",
    "            # Error handling for videos with disabled comments\n",
    "            # Got the answer format from StackOverflow (https://stackoverflow.com/questions/19342111/get-http-error-code-from-requests-exceptions-httperror)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments are disabled for the video with videoId: {vid['video_id']}\")\n",
    "                else:\n",
    "                    print(\"An HTTP error occurred:\", e)\n",
    "                # Continue to the next video\n",
    "                continue\n",
    "                \n",
    "        return vids_final\n",
    "    \n",
    "    all_comments = []\n",
    "    #for channel, upload_id in zip(channels, up_id):\n",
    "    for keyword_name, keywords in keyword_lists.items():\n",
    "        videos_info = keyword_videos('UUaXkIU1QidjPwiAYu6GcHjg', keywords, 'MSNBC')\n",
    "        video_comments = get_vid_comments(videos_info, limit)\n",
    "        all_comments.extend(video_comments)\n",
    "    \n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Democratic YouTube Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vice_comments = get_video_comments('Vice', 'UUn8zNIfYAQNdrFRrr8oibKw', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "vice_comments_df = pd.DataFrame(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vice_comments_df.to_csv(\"vice_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_comments = get_video_comments('Vox', 'UULXo7UDZvByw2ixzpQCufnA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "vox_comments_df = pd.DataFrame(vox_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vox_comments_df.to_csv(\"vox_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_comments = get_video_comments('MSNBC', 'UUaXkIU1QidjPwiAYu6GcHjg', keyword_lists, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "msnbc_comments_df = pd.DataFrame(msnbc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "msnbc_comments_df.to_csv(\"msnbc_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Daily Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyshow_comments = get_video_comments('The Daily Show', 'UUwWhs_6x42TyRM4Wstoq8HA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "dailyshow_comments_df = pd.DataFrame(dailyshow_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "dailyshow_comments_df.to_csv(\"dailyshow_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Younn Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments = get_video_comments('The Young Turks', 'UU1yBKRuGpC1tSM73A0ZjYjQ', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "yturk_comments_df = pd.DataFrame(yturk_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "yturk_comments_df.to_csv(\"yturk_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Republican YouTube Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Playlst ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets list of Right channels playlist id for uploads\n",
    "right_up_id = []\n",
    "for channel in channels_right:\n",
    "    chan_id = get_channel_id(channel)\n",
    "    upload_id = get_upload_id(chan_id)\n",
    "    right_up_id.append(upload_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get YouTube Video Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collects video titles for each of the given channels that contain keywords given\n",
    "right_video_titles = []\n",
    "for channel, upload_id in zip(channels_right, right_up_id):\n",
    "    print(channel)\n",
    "    videos_info = keyword_videos_right(upload_id, channel, right_video_titles)\n",
    "\n",
    "right_df = pd.DataFrame(right_video_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV\n",
    "right_df.to_csv('Project_yt_titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting comments for a given of videos\n",
    "def get_vid_comments_right(vid, limit):\n",
    "    vids_final = []\n",
    "    \n",
    "    # Iterate through each video in the video list\n",
    "    request = youtube.commentThreads().list(\n",
    "        videoId=vid['id'],\n",
    "        part='id,snippet,replies',\n",
    "        textFormat='plainText',\n",
    "        order='relevance',\n",
    "        maxResults=100\n",
    "    )\n",
    "    res = request.execute()\n",
    "\n",
    "    # Iterate through each comment\n",
    "    try:\n",
    "        while res[\"nextPageToken\"] != None:\n",
    "            for v in res[\"items\"]:\n",
    "                # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                vid_temp = vid.copy()\n",
    "                vid_temp.update({'CommentId':v['id']})\n",
    "                vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                vids_final.append(vid_temp)\n",
    "            \n",
    "            request = youtube.commentThreads().list(\n",
    "                videoId=vid['id'],\n",
    "                part='id,snippet,replies',\n",
    "                textFormat='plainText',\n",
    "                order='relevance',\n",
    "                maxResults=100,\n",
    "                pageToken = res[\"nextPageToken\"]\n",
    "            )\n",
    "            res = request.execute()\n",
    "    except KeyError:\n",
    "        for v in res[\"items\"]:\n",
    "                # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                vid_temp = vid.copy()\n",
    "                vid_temp.update({'CommentId':v['id']})\n",
    "                vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                vids_final.append(vid_temp)\n",
    "        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "        if len(vids_final) >= limit:\n",
    "            return(vids_final)\n",
    "            \n",
    "            \n",
    "    return vids_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Runs code to get comments for each title in right_video_titles\n",
    "right_comments_dict_list = []\n",
    "for title in right_video_titles:\n",
    "    result = get_vid_comments_right(title, 100)\n",
    "    right_comments_dict_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts list of list of dictionaries to a flat list\n",
    "flat_list_of_dicts = [item for sublist in right_comments_dict_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts list of dictionaries to dataframe\n",
    "right_comments_df = pd.DataFrame(flat_list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV\n",
    "right_comments_df.to_csv('Project_yt_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\junk\\ipykernel_12244\\1937675395.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  right_comment_df = pd.read_csv('Project_yt_comments.csv')\n"
     ]
    }
   ],
   "source": [
    "right_comment_df = pd.read_csv('Project_yt_comments.csv')\n",
    "right_title_df = pd.read_csv('Project_yt_titles.csv')\n",
    "demo_df = pd.read_csv('combine_democ_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "        #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN\n",
    "right_comment_df = right_comment_df.dropna()\n",
    "right_title_df = right_title_df.dropna()\n",
    "demo_df = demo_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from datetime to date\n",
    "right_comment_df['CommentCreationTime'] = right_comment_df['CommentCreationTime'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "right_title_df['published_at'] = right_title_df['published_at'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "demo_df['CommentCreationTime'] = demo_df['CommentCreationTime'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "demo_df['published_at'] = demo_df['published_at'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "right_comment_df['TweetToken'] = right_comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "right_title_df['TweetToken'] = right_title_df['title'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "demo_df['TweetTokenTitle'] = demo_df['title'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "demo_df['TweetTokenComment'] = demo_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "right_comment_df['CommentCleaned'] = right_comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "right_title_df['TitleCleaned'] = right_title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "demo_df['TitleCleaned'] = demo_df['TweetTokenTitle'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "demo_df['CommentCleaned'] = demo_df['TweetTokenComment'].apply(lambda x: remove_stop(textcleaner(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrc_sen(text, cat):\n",
    "    sen = NRCLex(text)\n",
    "    if cat == 'pos':\n",
    "        return sen.affect_frequencies['positive']\n",
    "    else:\n",
    "        return sen.affect_frequencies['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_comment_df['PositiveScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "right_comment_df['NegativeScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        \n",
    "right_title_df['PositiveScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "right_title_df['NegativeScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        \n",
    "\n",
    "demo_df['PositiveScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "demo_df['NegativeScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'neg'))    \n",
    "demo_df['PositiveScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'pos'))    \n",
    "demo_df['NegativeScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrc_emo(text, ver):\n",
    "    emo = NRCLex(text).affect_frequencies\n",
    "    max_emo = max(emo, key=emo.get)\n",
    "    max_score = emo[max_emo]\n",
    "    if ver == 'score':\n",
    "        return max_score\n",
    "    else:\n",
    "        return max_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_comment_df['Emotion'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "right_comment_df['EmotionScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'score'))        \n",
    "right_title_df['Emotion'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "right_title_df['EmotionScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'score'))        \n",
    "\n",
    "demo_df['EmotionTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "demo_df['EmotionScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'score'))    \n",
    "demo_df['EmotionComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "demo_df['EmotionScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'score'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "      <th>TweetTokenTitle</th>\n",
       "      <th>...</th>\n",
       "      <th>TitleCleaned</th>\n",
       "      <th>CommentCleaned</th>\n",
       "      <th>PositiveScoreTitle</th>\n",
       "      <th>NegativeScoreTitle</th>\n",
       "      <th>PositiveScoreComment</th>\n",
       "      <th>NegativeScoreComment</th>\n",
       "      <th>EmotionTitle</th>\n",
       "      <th>EmotionScoreTitle</th>\n",
       "      <th>EmotionComment</th>\n",
       "      <th>EmotionScoreComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>Ugws1dFQrp7AovnexrB4AaABAg</td>\n",
       "      <td>Bless the hard work of journalists! Seeing the...</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>146</td>\n",
       "      <td>[We, Uncovered, an, ISIS, Mass, Grave, |, Supe...</td>\n",
       "      <td>...</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>bless hard work journalists seeing deplorable ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgxcNLZW2rAeMBklWD14AaABAg</td>\n",
       "      <td>Also I can't imagine  the amount mental trauma...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>726</td>\n",
       "      <td>[We, Uncovered, an, ISIS, Mass, Grave, |, Supe...</td>\n",
       "      <td>...</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>also cant imagine amount mental trauma work pu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgzFcqbEHILJ93hjvqh4AaABAg</td>\n",
       "      <td>This is so heartbreaking. What a horrific disp...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>251</td>\n",
       "      <td>[We, Uncovered, an, ISIS, Mass, Grave, |, Supe...</td>\n",
       "      <td>...</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>heartbreaking horrific display violence sorry ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgxYrGf2RJZeKbFEap14AaABAg</td>\n",
       "      <td>7:35 Social-media shouldn't be just summarily ...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>219</td>\n",
       "      <td>[We, Uncovered, an, ISIS, Mass, Grave, |, Supe...</td>\n",
       "      <td>...</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>socialmedia shouldnt summarily deleting flagge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>Ugx5rmcME6ua2pMkojt4AaABAg</td>\n",
       "      <td>VICE NEVER Dissapoints! Amazing documentaries!...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>127</td>\n",
       "      <td>[We, Uncovered, an, ISIS, Mass, Grave, |, Supe...</td>\n",
       "      <td>...</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>vice never dissapoints amazing documentaries t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel     video_id                                          title keyword  \\\n",
       "0    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "1    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "2    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "3    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "4    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "\n",
       "  published_at                   CommentId  \\\n",
       "0   2022-04-11  Ugws1dFQrp7AovnexrB4AaABAg   \n",
       "1   2022-04-11  UgxcNLZW2rAeMBklWD14AaABAg   \n",
       "2   2022-04-11  UgzFcqbEHILJ93hjvqh4AaABAg   \n",
       "3   2022-04-11  UgxYrGf2RJZeKbFEap14AaABAg   \n",
       "4   2022-04-11  Ugx5rmcME6ua2pMkojt4AaABAg   \n",
       "\n",
       "                                        CommentTitle CommentCreationTime  \\\n",
       "0  Bless the hard work of journalists! Seeing the...          2022-04-12   \n",
       "1  Also I can't imagine  the amount mental trauma...          2022-04-11   \n",
       "2  This is so heartbreaking. What a horrific disp...          2022-04-11   \n",
       "3  7:35 Social-media shouldn't be just summarily ...          2022-04-11   \n",
       "4  VICE NEVER Dissapoints! Amazing documentaries!...          2022-04-11   \n",
       "\n",
       "   CommentLikes                                    TweetTokenTitle  ...  \\\n",
       "0           146  [We, Uncovered, an, ISIS, Mass, Grave, |, Supe...  ...   \n",
       "1           726  [We, Uncovered, an, ISIS, Mass, Grave, |, Supe...  ...   \n",
       "2           251  [We, Uncovered, an, ISIS, Mass, Grave, |, Supe...  ...   \n",
       "3           219  [We, Uncovered, an, ISIS, Mass, Grave, |, Supe...  ...   \n",
       "4           127  [We, Uncovered, an, ISIS, Mass, Grave, |, Supe...  ...   \n",
       "\n",
       "                            TitleCleaned  \\\n",
       "0  uncovered isis mass grave super users   \n",
       "1  uncovered isis mass grave super users   \n",
       "2  uncovered isis mass grave super users   \n",
       "3  uncovered isis mass grave super users   \n",
       "4  uncovered isis mass grave super users   \n",
       "\n",
       "                                      CommentCleaned PositiveScoreTitle  \\\n",
       "0  bless hard work journalists seeing deplorable ...                0.0   \n",
       "1  also cant imagine amount mental trauma work pu...                0.0   \n",
       "2  heartbreaking horrific display violence sorry ...                0.0   \n",
       "3  socialmedia shouldnt summarily deleting flagge...                0.0   \n",
       "4  vice never dissapoints amazing documentaries t...                0.0   \n",
       "\n",
       "   NegativeScoreTitle  PositiveScoreComment  NegativeScoreComment  \\\n",
       "0            0.333333              0.066667              0.133333   \n",
       "1            0.333333              0.230769              0.076923   \n",
       "2            0.333333              0.000000              0.222222   \n",
       "3            0.333333              0.166667              0.166667   \n",
       "4            0.333333              0.285714              0.071429   \n",
       "\n",
       "   EmotionTitle EmotionScoreTitle  EmotionComment EmotionScoreComment  \n",
       "0          fear          0.333333            fear            0.200000  \n",
       "1          fear          0.333333        positive            0.230769  \n",
       "2          fear          0.333333            fear            0.222222  \n",
       "3          fear          0.333333            fear            0.166667  \n",
       "4          fear          0.333333        positive            0.285714  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
