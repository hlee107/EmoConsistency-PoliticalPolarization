{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05df723",
   "metadata": {},
   "source": [
    "# Emotional Consistency among Political Ideologies: An Approach to Address Polarization on Youtube\n",
    "\n",
    "## Group 5\n",
    "### Chance Landis (ChancL), Hanna Lee (Lee10), Jason Sun (yongxs), Andy Wong (WongA22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyAJ_CYX7COa6ZHx0vnvb2rVxlyKp91B3lE\"\n",
    "!pip install --upgrade google-api-python-client --quiet\n",
    "import json\n",
    "\n",
    "import googleapiclient\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n",
    "import re\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c68a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id of a channel\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(part='contentDetails', forUsername=channel)\n",
    "    res = request.execute()\n",
    "    return res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "# Function for retrieving all vids within the upload playlist of a channel, stopping once a limit INT has been reached\n",
    "def get_vids(channel, limit, keywords, ideology):\n",
    "    \n",
    "    # Output list\n",
    "    vid_lst=[]\n",
    "\n",
    "    request = youtube.playlistItems().list(part='snippet',playlistId=get_upload_id(channel),maxResults=50)\n",
    "        \n",
    "    res = request.execute()\n",
    "    nextPageToken = res['nextPageToken']\n",
    "\n",
    "    # Iterate through each video in the playlist\n",
    "    for v in res[\"items\"]:\n",
    "\n",
    "        # Normalization of video title to check for keywords\n",
    "        title = v['snippet']['title']\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "        # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "        counter = 0\n",
    "        for word in title.split():\n",
    "            counter = 0\n",
    "            if word in keywords:\n",
    "                counter += 1\n",
    "        if counter == 0:\n",
    "            continue\n",
    "\n",
    "        # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "        vid_dict = {}\n",
    "        vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "        vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "        vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "        vid_dict['Ideology'] = ideology\n",
    "\n",
    "        # Separate Resource Call to retrieve video views\n",
    "        views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "        view_temp = views.execute()\n",
    "        vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "        # Append dictionary to greater list\n",
    "        vid_lst.append(vid_dict)\n",
    "\n",
    "    # Iterate until no more next page\n",
    "    while nextPageToken:\n",
    "        try:\n",
    "            request = youtube.playlistItems().list(part='snippet', playlistId=get_upload_id(channel), maxResults=50, pageToken = res['nextPageToken'])                \n",
    "            res = request.execute()\n",
    "\n",
    "            # Redefine next page token to check @ next iteration\n",
    "            nextPageToken = res['nextPageToken']\n",
    "\n",
    "            # Iterate through each video\n",
    "            for v in res[\"items\"]:\n",
    "\n",
    "                # Normalization of video title to check for keywords\n",
    "                title = v['snippet']['title']\n",
    "                title = title.lower()\n",
    "                title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "                # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "                counter = 0\n",
    "                for word in title.split():\n",
    "                    if word in keywords:\n",
    "                        counter += 1\n",
    "                if counter == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "                vid_dict = {}\n",
    "                vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "                vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "                vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "                \n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "                \n",
    "                vid_lst.append(vid_dict)\n",
    "\n",
    "            # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "            if len(vid_lst) >= limit:\n",
    "                return(vid_lst)\n",
    "\n",
    "        # Error case handling\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "# Function for getting top 30 relevant comments for a list of videos\n",
    "def get_vid_comments(vid_lst, limit):\n",
    "    vids_final = []\n",
    "\n",
    "    # Iterate through each video in the video list\n",
    "    for vid in vid_lst:\n",
    "        \n",
    "        request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "        res = request.execute()\n",
    "\n",
    "        # Iterate through each comment\n",
    "        for v in res[\"items\"]:\n",
    "            \n",
    "            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "            vid_temp = copy.copy(vid)\n",
    "            vid_temp.update({'CommentId':v['id']})\n",
    "            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "            vids_final.append(vid_temp)\n",
    "\n",
    "        while nextPageToken:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "                res = request.execute()\n",
    "        \n",
    "                nextPageToken = res['nextPageToken']\n",
    "                \n",
    "                for v in res[\"items\"]:\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = copy.copy(vid)\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "                    \n",
    "                # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                if len(vids_final) >= limit:\n",
    "                    return(vids_final)\n",
    "            except KeyError:\n",
    "                break\n",
    "            \n",
    "    return vids_final\n",
    "\n",
    "# from Lab9\n",
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "    #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')\n",
    "\n",
    "def df_clean_process(df):\n",
    "\n",
    "    # Change datetime to date\n",
    "    df['VideoPublishedDate'] = df['VideoPublishedDate'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "    df['CommentCreationTime'] = df['CommentCreationTime'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "\n",
    "    # Check NaN, if < 10% of total dataset, drop NaN\n",
    "    if df.isnull().values.any():\n",
    "        if len(df[df.isna().any(axis=1)]) < len(df) * 0.1:\n",
    "            df = df.dropna()\n",
    "\n",
    "    # Split into separate df for computational load reduction\n",
    "    title_df = df[['ChannelName', 'VideoTitle', 'VideoPublishedDate', 'VideoViews', 'Ideology']].drop_duplicates()\n",
    "    comment_df = df[['ChannelName', 'VideoViews', 'CommentTitle', 'CommentCreationTime', 'CommentLikes', 'Ideology']]\n",
    "\n",
    "    # tokenize\n",
    "    title_df['TweetToken'] = title_df['VideoTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "    comment_df['TweetToken'] = comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "\n",
    "    # clean\n",
    "    title_df['Cleaned'] = title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "    comment_df['Cleaned'] = comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "\n",
    "    return (title_df, comment_df)\n",
    "\n",
    "    # Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define channels\n",
    "channels_left = ['VICE', 'Vox', 'MSNBC', 'The Daily Show', 'TheYoungTurks']\n",
    "channels_right = ['Fox News', 'Ben Shapiro', 'StevenCrowder', 'Daily Mail', 'DailyWire+']\n",
    "\n",
    "# define key ideologies/associated keywords to look for in title\n",
    "isis_keywords = ['terrorism', 'terrorist', 'extremism', 'radicalist', 'radicalism']\n",
    "guns_keywords = ['shooting', 'shootings', 'school shooting', 'school shootings', 'firearms', 'firearm', 'gun', 'gun control', 'guns', 'nra', 'second amendment']\n",
    "immigration_keywords = ['border control', 'mexico', 'visa', 'citizenship', 'asylum', 'deportation', 'refugee']\n",
    "economy_keywords = ['budget', 'budget deficit', 'unemployed', 'inflation', 'interest rate',' federal reserve', 'market', 'employment']\n",
    "health_care_keywords = ['medicaid', 'covid', 'obamacare', 'public health', 'insurance']\n",
    "socioeconomic_keywords = ['rich', 'poor', 'income inequality', 'poverty',' wealth distribution']\n",
    "abortion_keywords = ['pregnancy', 'unwanted pregnancy', 'roe', 'wade', 'abortion', 'pro-life', 'rape', 'incest', 'life of mother', 'religion']\n",
    "climate_change_keywords = ['global warming', 'carbon', 'alternative energy', 'climate', 'methane', 'emissions','gas','greenhouse']\n",
    "\n",
    "# Define for iteration\n",
    "keywords = [isis_keywords, guns_keywords, immigration_keywords, economy_keywords, health_care_keywords, socioeconomic_keywords, abortion_keywords, climate_change_keywords]\n",
    "\n",
    "# Pre-define empty df\n",
    "left_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "\n",
    "# Loop through all left channels\n",
    "for channel in channels_left:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        left_df = pd.concat([left_df,temp_df])\n",
    "\n",
    "# Pre-define empty df\n",
    "right_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "\n",
    "# Loop through all right channels\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9380f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
