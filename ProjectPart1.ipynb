{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5280fa8",
   "metadata": {},
   "source": [
    "# Emotional Consistency among Political Ideologies: An Approach to Address Polarization on Youtube\n",
    "\n",
    "Group 5:\n",
    "- Chance Landis (ChancL), Hanna Lee (Lee10), Jason Sun (YongXs), Andy Wong (WongA22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3b213",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca127c6",
   "metadata": {},
   "source": [
    "### Sources of Information\n",
    "- **AllSides**: A media bias tool that provides a rating based on \"multi-partisan Editorial Reviews by trained experts and Blind Bias Surveys™ in which participants rate content without knowing the source.\" We used this tool to determine how we should classify the most popular (based on subscriber count) YouTube channels we found. (Source: https://www.allsides.com/media-bias/media-bias-rating-methods)\n",
    "- **HypeAuitor**: A company that uses a data-driven approach to influencer marketing. In the process, they collated lists of YouTube based on category, subscriber count, and country. This allowed us to find YouTube channels that focused on news and politics with the most subscribers. (Source: https://hypeauditor.com/about/company/, https://hypeauditor.com/top-youtube-news-politics-united-states/)\n",
    "- **Pew Research Center**: A nonpartisan, nonprofit organization that conducts research on public opinion, demographic trends, and social issues. It provides data-driven insights into various aspects of social science issues, explicitly stating they do not take a stance on political issues. For our research, we relied on their studies on political ideologies and alignment with political parties as a reference. (Source: https://www.pewresearch.org/about/, https://www.pewresearch.org/politics/2016/06/22/5-views-of-parties-positions-on-issues-ideologies/)\n",
    "- **YouTube**: As a group, we've chosen to expand our collection of YouTube videos by selecting additional keywords associated with the ideology we're studying. Our focus will be on gathering comments from these videos to conduct our research.\n",
    "    - We used a combination of Andy and Hanna's code to get the comments from YouTube channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5077c5",
   "metadata": {},
   "source": [
    "### Top 5 Democratic YouTube Channels\n",
    "Vice, Vox, MSNBC, The Daily Show, The Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e0982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\ospython\\lib\\site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 14.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd25be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a904e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.5/1.5 MB 14.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 24.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.5/269.5 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1891f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1a054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.0-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/11.6 MB 13.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 15.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.6/11.6 MB 20.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.6/11.6 MB 20.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.6/11.6 MB 20.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.7/11.6 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.7/11.6 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.7/11.6 MB 21.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/11.6 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/11.6 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.4/11.6 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 19.8 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.1/15.8 MB 33.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.5/15.8 MB 31.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.9/15.8 MB 30.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 3.1/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.3/15.8 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 4.0/15.8 MB 14.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.8/15.8 MB 15.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.9/15.8 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.7/15.8 MB 16.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 7.1/15.8 MB 15.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.3/15.8 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.8/15.8 MB 17.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.1/15.8 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.2/15.8 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/15.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.8 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 505.5/505.5 kB 16.0 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 345.4/345.4 kB 20.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.0 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef81ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nrclex\n",
      "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
      "Collecting textblob (from nrclex)\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "INFO: pip is looking at multiple versions of nrclex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nrclex\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "     ---------------------------------------- 0.0/396.4 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 286.7/396.4 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 396.4/396.4 kB 6.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.8 in d:\\ospython\\lib\\site-packages (from textblob->nrclex) (3.8.1)\n",
      "Requirement already satisfied: click in d:\\ospython\\lib\\site-packages (from nltk>=3.8->textblob->nrclex) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\ospython\\lib\\site-packages (from nltk>=3.8->textblob->nrclex) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\ospython\\lib\\site-packages (from nltk>=3.8->textblob->nrclex) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in d:\\ospython\\lib\\site-packages (from nltk>=3.8->textblob->nrclex) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\me\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk>=3.8->textblob->nrclex) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 626.3/626.3 kB 19.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: nrclex\n",
      "  Building wheel for nrclex (pyproject.toml): started\n",
      "  Building wheel for nrclex (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43340 sha256=d33f7512c26a0d5d6b28b9a56ce37bb3c9686b93d53955f297121b38d32638e2\n",
      "  Stored in directory: c:\\users\\me\\appdata\\local\\pip\\cache\\wheels\\d2\\10\\44\\6abfb1234298806a145fd6bcaec8cbc712e88dd1cd6cb242fa\n",
      "Successfully built nrclex\n",
      "Installing collected packages: textblob, nrclex\n",
      "Successfully installed nrclex-3.0.0 textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install nrclex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ebc599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stopwords\n",
      "  Downloading stopwords-1.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: stopwords\n",
      "Successfully installed stopwords-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb4dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import googleapiclient\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nrclex import NRCLex\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15c3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call\n",
    "# vice: AIzaSyA2rNi_MI-3LQkBzzQ6Tn4EF0lgXWoilfc\n",
    "# vox: AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao\n",
    "# msnbc: AIzaSyBZDxP2HEW50EDfExcZJag7J2mRroZ9_vk\n",
    "# daily show: AIzaSyD8adQZlhLNVQrQXpU5-u3s1Y-9TZs20ik\n",
    "# young turk: AIzaSyB8yyrUrfQGLrlQRmF555oc1emrIDXF7yU\n",
    "\n",
    "# Others: API_KEY = \"AIzaSyCjWja_yyRROSw5tcP_KxYjasJgHLX3oKE\"\n",
    "# API_KEY = \"AIzaSyCjWja_yyRROSw5tcP_KxYjasJgHLX3oKE\"\n",
    "API_KEY = \"AIzaSyCfjrHtWz-ySCQMobOW0DrN3IwvIZL_YEE\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926a78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels\n",
    "channels = [\"Vice\", \"Vox\", \"msnbc\", \"thedailyshow\", \"TheYoungTurks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d8c1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords\n",
    "keyword_lists = {\n",
    "    \"isis\": [\"ISIS\", \"Terrorism\", \"Radicalist\", \"Jihad\", \"Suicide Bombing\"],\n",
    "    \"guns\": [\"Gun\", \"Shooting\", \"School shooting\", \"Firearm\", \"Gun control\", \"NRA\", \"Second Amendment\"],\n",
    "    \"immigration\": [\"Immigration\", \"Border control\", \"Mexico\", \"Visa\", \"Citizenship\", \"Asylum\", \"Deportation\", \"Refugee\"],\n",
    "    \"economy\": [\"Economy\", \"Budget deficit\", \"Unemployed\", \"Inflation\", \"Interest rate\", \"Federal reserve\", \"Market\", \"Employment\"],\n",
    "    \"healthcare\": [\"Health care\", \"Medicaid\", \"Covid\", \"Obamacare\", \"Public health\", \"Insurance\"],\n",
    "    \"socioeco\": [\"Socio-economic\", \"Rich\", \"Poor\", \"Income inequality\", \"Poverty\", \"Wealth distribution\"],\n",
    "    \"abortion\": [\"Abortion\", \"Pregnancy\", \"Unwanted Pregnancy\", \"Roe\", \"Wade\", \"Pro-life\", \"Rape\", \"Incest\", \"Life of mother\", \"Religion\"],\n",
    "    \"climate\": [\"Climate change\", \"Global Warming\", \"Carbon\", \"Alternative Energy\", \"Climate\", \"Methane\", \"Emissions\", \"Gas\", \"Greenhouse\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381766a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting channel id based on name\n",
    "def get_channel_id(channel):  \n",
    "    channel_id = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        type=\"channel\",\n",
    "        q=channel\n",
    "    )\n",
    "\n",
    "    res_channel = channel_id.execute()\n",
    "    chan_id = res_channel[\"items\"][0][\"id\"][\"channelId\"]\n",
    "\n",
    "    return chan_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5408ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id using channel id\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel\n",
    "    )\n",
    "\n",
    "    res = request.execute()\n",
    "    uploads_playlist_id = res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "    return uploads_playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beb100e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vice\n",
      "Vox\n",
      "msnbc\n",
      "thedailyshow\n",
      "TheYoungTurks\n"
     ]
    }
   ],
   "source": [
    "up_id = []\n",
    "\n",
    "for channel in channels:\n",
    "    print(channel)\n",
    "    chan_id = get_channel_id(channel)\n",
    "    upload_id = get_upload_id(chan_id)\n",
    "    up_id.append(upload_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "015a246c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UUn8zNIfYAQNdrFRrr8oibKw',\n",
       " 'UULXo7UDZvByw2ixzpQCufnA',\n",
       " 'UUaXkIU1QidjPwiAYu6GcHjg',\n",
       " 'UUwWhs_6x42TyRM4Wstoq8HA',\n",
       " 'UU1yBKRuGpC1tSM73A0ZjYjQ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601b31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Function to check if a video title contains any of the keywords\n",
    "def contains_keyword(title, keywords):\n",
    "    title_lower = title.lower()\n",
    "    words = word_tokenize(title_lower)\n",
    "    \n",
    "    # Stem each word in the title + keyword\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    for keyword in keywords:\n",
    "        keyword_stemmed = ps.stem(keyword.lower())\n",
    "        if keyword_stemmed in stemmed_words:\n",
    "            return keyword\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch videos from a playlist and get title with keywords\n",
    "def keyword_videos(playlist_id, keywords, channel_name):\n",
    "    videos_info = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Make the next API request using the nextPageToken\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=next_page_token\n",
    "        ) \n",
    "        res = request.execute()\n",
    "\n",
    "        # Process the response and save video info\n",
    "        for v in res[\"items\"]:\n",
    "            video_title = v[\"snippet\"][\"title\"]\n",
    "            detected_word = contains_keyword(video_title, keywords)\n",
    "            if detected_word:\n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                video_views = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "                # Append video information with views to videos_info list\n",
    "                videos_info.append({\n",
    "                    \"id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                    \"title\": video_title,\n",
    "                    \"keyword\": detected_word,\n",
    "                    \"published_at\": v[\"snippet\"][\"publishedAt\"],\n",
    "                    \"VideoViews\": video_views\n",
    "                })\n",
    "        # Update the nextPageToken for the next iteration\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token or (len(videos_info) > 30):\n",
    "            break\n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for channel, upload_id in zip(channels, up_id):\n",
    "#     for keyword_name, keywords in keyword_lists.items():\n",
    "#         videos_info = keyword_videos('UUn8zNIfYAQNdrFRrr8oibKw', keywords, 'Vice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7926b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(channels, up_id, keyword_lists, limit=30):\n",
    "    # Function to fetch videos from a playlist and get title with keywordsand \n",
    "    def keyword_videos(playlist_id, keywords, channel_name):\n",
    "        videos_info = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            # Make the next API request using the nextPageToken\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                pageToken=next_page_token\n",
    "            ) \n",
    "            res = request.execute()\n",
    "\n",
    "            # Process the response and save video info\n",
    "            for v in res[\"items\"]:\n",
    "                video_title = v[\"snippet\"][\"title\"]\n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    videos_info.append(\n",
    "                    {\n",
    "                        \"channel\": channel_name,\n",
    "                        \"video_id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"]\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "            # Update the nextPageToken for the next iteration\n",
    "            next_page_token = res.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token or (len(videos_info) > 15):\n",
    "                break\n",
    "        return videos_info\n",
    "\n",
    "    # Function for getting top 30 relevant comments for a list of videos\n",
    "    def get_vid_comments(vid_lst, limit):\n",
    "        vids_final = []\n",
    "\n",
    "        # Iterate through each video in the video list\n",
    "        for vid in vid_lst:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    videoId=vid['video_id'],\n",
    "                    part='id,snippet,replies',\n",
    "                    textFormat='plainText',\n",
    "                    order='relevance',\n",
    "                    maxResults=50)\n",
    "                res = request.execute()\n",
    "\n",
    "                # Iterate through each comment\n",
    "                for v in res[\"items\"]:\n",
    "\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = vid.copy()\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "\n",
    "                nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                while nextPageToken:\n",
    "                    try:\n",
    "                        request = youtube.commentThreads().list(\n",
    "                            videoId=vid['video_id'],\n",
    "                            part='id,snippet,replies',\n",
    "                            textFormat='plainText',\n",
    "                            order='relevance',\n",
    "                            maxResults=50,\n",
    "                            pageToken=nextPageToken)\n",
    "\n",
    "                        res = request.execute()\n",
    "\n",
    "                        nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                        for v in res[\"items\"]:\n",
    "                            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                            vid_temp = vid.copy()\n",
    "                            vid_temp.update({'CommentId':v['id']})\n",
    "                            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                            vids_final.append(vid_temp)\n",
    "\n",
    "                        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                        if len(vids_final) >= limit:\n",
    "                            return vids_final\n",
    "                    except KeyError:\n",
    "                        break\n",
    "\n",
    "            # Error handling for videos with disabled comments\n",
    "            # Got the answer format from StackOverflow (https://stackoverflow.com/questions/19342111/get-http-error-code-from-requests-exceptions-httperror)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments are disabled for the video with videoId: {vid['video_id']}\")\n",
    "                else:\n",
    "                    print(\"An HTTP error occurred:\", e)\n",
    "                # Continue to the next video\n",
    "                continue\n",
    "                \n",
    "        return vids_final\n",
    "    \n",
    "    all_comments = []\n",
    "    #for channel, upload_id in zip(channels, up_id):\n",
    "    for keyword_name, keywords in keyword_lists.items():\n",
    "        videos_info = keyword_videos('UUaXkIU1QidjPwiAYu6GcHjg', keywords, 'MSNBC')\n",
    "        video_comments = get_vid_comments(videos_info, limit)\n",
    "        all_comments.extend(video_comments)\n",
    "    \n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d171dd4",
   "metadata": {},
   "source": [
    "#### Vice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75b3444f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments are disabled for the video with videoId: EEIvWNhuL8U\n"
     ]
    }
   ],
   "source": [
    "vice_comments = get_video_comments('Vice', 'UUn8zNIfYAQNdrFRrr8oibKw', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35c42f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8cee02f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugws1dFQrp7AovnexrB4AaABAg',\n",
       "  'CommentTitle': 'Bless the hard work of journalists! Seeing the deplorable and terrible things done by monstrous groups like ISIS in one spot must be so difficult. We’re with you!',\n",
       "  'CommentCreationTime': '2022-04-12T22:53:43Z',\n",
       "  'CommentLikes': 146},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxcNLZW2rAeMBklWD14AaABAg',\n",
       "  'CommentTitle': \"Also I can't imagine  the amount mental trauma this work puts these journalists and their teams  undergo having to file through hours of footage of some of the most horrific acts enacted upon people in order to try and piece together what really happened.  If they can uncover even some of truth that is a very big step forward to those victims who are still alive and hopefullt it can highlight those respsonsible\",\n",
       "  'CommentCreationTime': '2022-04-11T16:49:08Z',\n",
       "  'CommentLikes': 726},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzFcqbEHILJ93hjvqh4AaABAg',\n",
       "  'CommentTitle': 'This is so heartbreaking. What a horrific display of violence. I’m so sorry this happened to these people. I’m praying for their families 🥺🙏🏽',\n",
       "  'CommentCreationTime': '2022-04-11T15:08:45Z',\n",
       "  'CommentLikes': 251},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxYrGf2RJZeKbFEap14AaABAg',\n",
       "  'CommentTitle': \"7:35 Social-media shouldn't be just summarily deleting flagged content when they remove stuff, they should be quarantining it, forwarding it to law-enforcement and making it available for investigators to requisition. We know they never delete any data anyway, so it's not like the media is actually _gone_ so it SHOULD still be available for the relevant authorities. Problem solved.\",\n",
       "  'CommentCreationTime': '2022-04-11T15:59:18Z',\n",
       "  'CommentLikes': 219},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugx5rmcME6ua2pMkojt4AaABAg',\n",
       "  'CommentTitle': 'VICE NEVER Dissapoints! Amazing documentaries! Thank you 🙏🏽 for your amazing humanity rights findings not only for one race or culture but for everyone god bless!! 🙌',\n",
       "  'CommentCreationTime': '2022-04-11T15:04:32Z',\n",
       "  'CommentLikes': 127},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzzbQ-K80UX_H02FPR4AaABAg',\n",
       "  'CommentTitle': 'Surely if social media companies pull such content they need to forward onto investigators as part of their due diligence in terms of business-human rights.',\n",
       "  'CommentCreationTime': '2022-04-11T15:14:45Z',\n",
       "  'CommentLikes': 161},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgysAY65Ywm8Z54rrMZ4AaABAg',\n",
       "  'CommentTitle': 'Wow, this was interesting and I learned some things I didn’t know before. Thanks for “shedding light so others can see”, hey I like that! I appreciate good, honest journalism. Props. 👊',\n",
       "  'CommentCreationTime': '2022-04-11T18:23:50Z',\n",
       "  'CommentLikes': 33},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugwk0pK1S0kCyDg-QoZ4AaABAg',\n",
       "  'CommentTitle': 'Another great video by VICE, sometimes u guys produce mediocre content, but a lot of great stuff... Been following since the older documentary days about north korea',\n",
       "  'CommentCreationTime': '2022-04-12T05:37:50Z',\n",
       "  'CommentLikes': 93},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz8RjIShFG5SQ6XvyZ4AaABAg',\n",
       "  'CommentTitle': 'You guys are literally next level heros!!',\n",
       "  'CommentCreationTime': '2022-04-17T01:07:35Z',\n",
       "  'CommentLikes': 5},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz1DoNZkAejBAok2Xd4AaABAg',\n",
       "  'CommentTitle': 'You guys r the most realistic, journalism i have seen. \\nThank you so much for bringing this',\n",
       "  'CommentCreationTime': '2022-04-11T16:59:17Z',\n",
       "  'CommentLikes': 4}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vice_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8a1c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "vice_comments_df = pd.DataFrame(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e8ff823",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>Ugws1dFQrp7AovnexrB4AaABAg</td>\n",
       "      <td>Bless the hard work of journalists! Seeing the...</td>\n",
       "      <td>2022-04-12T22:53:43Z</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgxcNLZW2rAeMBklWD14AaABAg</td>\n",
       "      <td>Also I can't imagine  the amount mental trauma...</td>\n",
       "      <td>2022-04-11T16:49:08Z</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgzFcqbEHILJ93hjvqh4AaABAg</td>\n",
       "      <td>This is so heartbreaking. What a horrific disp...</td>\n",
       "      <td>2022-04-11T15:08:45Z</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgxYrGf2RJZeKbFEap14AaABAg</td>\n",
       "      <td>7:35 Social-media shouldn't be just summarily ...</td>\n",
       "      <td>2022-04-11T15:59:18Z</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>Ugx5rmcME6ua2pMkojt4AaABAg</td>\n",
       "      <td>VICE NEVER Dissapoints! Amazing documentaries!...</td>\n",
       "      <td>2022-04-11T15:04:32Z</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel     video_id                                          title keyword  \\\n",
       "0    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "1    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "2    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "3    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "4    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "\n",
       "           published_at                   CommentId  \\\n",
       "0  2022-04-11T15:00:12Z  Ugws1dFQrp7AovnexrB4AaABAg   \n",
       "1  2022-04-11T15:00:12Z  UgxcNLZW2rAeMBklWD14AaABAg   \n",
       "2  2022-04-11T15:00:12Z  UgzFcqbEHILJ93hjvqh4AaABAg   \n",
       "3  2022-04-11T15:00:12Z  UgxYrGf2RJZeKbFEap14AaABAg   \n",
       "4  2022-04-11T15:00:12Z  Ugx5rmcME6ua2pMkojt4AaABAg   \n",
       "\n",
       "                                        CommentTitle   CommentCreationTime  \\\n",
       "0  Bless the hard work of journalists! Seeing the...  2022-04-12T22:53:43Z   \n",
       "1  Also I can't imagine  the amount mental trauma...  2022-04-11T16:49:08Z   \n",
       "2  This is so heartbreaking. What a horrific disp...  2022-04-11T15:08:45Z   \n",
       "3  7:35 Social-media shouldn't be just summarily ...  2022-04-11T15:59:18Z   \n",
       "4  VICE NEVER Dissapoints! Amazing documentaries!...  2022-04-11T15:04:32Z   \n",
       "\n",
       "   CommentLikes  \n",
       "0           146  \n",
       "1           726  \n",
       "2           251  \n",
       "3           219  \n",
       "4           127  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vice_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f86d93b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vice_comments_df.to_csv(\"vice_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7d9dc",
   "metadata": {},
   "source": [
    "### Vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f4b6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_comments = get_video_comments('Vox', 'UULXo7UDZvByw2ixzpQCufnA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6af35d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vox_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d1865eb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxCgDXiTTfjLBdkAE94AaABAg',\n",
       "  'CommentTitle': '\"It was a low degree of terror.\"\\nThis man could barely choke out those words a half century later...\\nLow degree?  I don\\'t think so.',\n",
       "  'CommentCreationTime': '2019-11-05T08:38:22Z',\n",
       "  'CommentLikes': 13004},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzpaYXLMtGz3ymErOJ4AaABAg',\n",
       "  'CommentTitle': 'Could you imagine having to question if every single person you engage with is a plant by some organization?',\n",
       "  'CommentCreationTime': '2019-11-04T13:16:13Z',\n",
       "  'CommentLikes': 7158},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgweQtEREv31yKgtumB4AaABAg',\n",
       "  'CommentTitle': 'That poor man never trusted another person and I feel so bad for him. I tear up every time he does. You can still see how hard he thinks before he speaks.',\n",
       "  'CommentCreationTime': '2019-11-30T14:30:31Z',\n",
       "  'CommentLikes': 4516},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwaArmK_X6VOrDrjrZ4AaABAg',\n",
       "  'CommentTitle': 'And this is only 1 person’s story.',\n",
       "  'CommentCreationTime': '2019-11-06T13:46:52Z',\n",
       "  'CommentLikes': 2228},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxzPYwa0EGXhdYhtMF4AaABAg',\n",
       "  'CommentTitle': '\"These interrogators, the investigators, they weren\\'t very bright people, but they had tremendous power over others. Over us.\" Some things absolutely never change',\n",
       "  'CommentCreationTime': '2019-11-05T02:56:20Z',\n",
       "  'CommentLikes': 3026},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxL6re97jEJbZIqxBZ4AaABAg',\n",
       "  'CommentTitle': 'Boomer: it was so much better when I was younger',\n",
       "  'CommentCreationTime': '2019-11-05T17:15:07Z',\n",
       "  'CommentLikes': 3903},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'Ugwv4GDXYpk7qiHTInZ4AaABAg',\n",
       "  'CommentTitle': '\"They weren\\'t very bright people but they have tremendous power over others, over us.\" Sound familiar?',\n",
       "  'CommentCreationTime': '2019-11-04T23:31:59Z',\n",
       "  'CommentLikes': 3657},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzDydG5LiAj1if6GvR4AaABAg',\n",
       "  'CommentTitle': 'I just want to offer a heartfelt thanks to the gentleman who was interviewed for this video. Sir, your courage and willingness to share about what was obviously a terrible time in your life (one that apparently affects you to this day) is invaluable to those of us who might otherwise have trouble connecting with that time and those events.',\n",
       "  'CommentCreationTime': '2019-11-04T14:09:00Z',\n",
       "  'CommentLikes': 11509},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwwabWjgasZiMUny_p4AaABAg',\n",
       "  'CommentTitle': 'I want to give that man a hug after what he went through',\n",
       "  'CommentCreationTime': '2019-11-04T14:27:25Z',\n",
       "  'CommentLikes': 3315},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgykYfLlDY9Gxiax96d4AaABAg',\n",
       "  'CommentTitle': 'Imagine hating someone because of their sexuality that has absolutely nothing to do with you.. lol',\n",
       "  'CommentCreationTime': '2020-07-10T15:23:49Z',\n",
       "  'CommentLikes': 2679}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vox_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12226f76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgxCgDXiTTfjLBdkAE94AaABAg</td>\n",
       "      <td>\"It was a low degree of terror.\"\\nThis man cou...</td>\n",
       "      <td>2019-11-05T08:38:22Z</td>\n",
       "      <td>13004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgzpaYXLMtGz3ymErOJ4AaABAg</td>\n",
       "      <td>Could you imagine having to question if every ...</td>\n",
       "      <td>2019-11-04T13:16:13Z</td>\n",
       "      <td>7158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgweQtEREv31yKgtumB4AaABAg</td>\n",
       "      <td>That poor man never trusted another person and...</td>\n",
       "      <td>2019-11-30T14:30:31Z</td>\n",
       "      <td>4516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgwaArmK_X6VOrDrjrZ4AaABAg</td>\n",
       "      <td>And this is only 1 person’s story.</td>\n",
       "      <td>2019-11-06T13:46:52Z</td>\n",
       "      <td>2228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgxzPYwa0EGXhdYhtMF4AaABAg</td>\n",
       "      <td>\"These interrogators, the investigators, they ...</td>\n",
       "      <td>2019-11-05T02:56:20Z</td>\n",
       "      <td>3026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel     video_id                                        title  \\\n",
       "0     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "1     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "2     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "3     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "4     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "\n",
       "     keyword          published_at                   CommentId  \\\n",
       "0  Terrorism  2019-11-04T13:00:06Z  UgxCgDXiTTfjLBdkAE94AaABAg   \n",
       "1  Terrorism  2019-11-04T13:00:06Z  UgzpaYXLMtGz3ymErOJ4AaABAg   \n",
       "2  Terrorism  2019-11-04T13:00:06Z  UgweQtEREv31yKgtumB4AaABAg   \n",
       "3  Terrorism  2019-11-04T13:00:06Z  UgwaArmK_X6VOrDrjrZ4AaABAg   \n",
       "4  Terrorism  2019-11-04T13:00:06Z  UgxzPYwa0EGXhdYhtMF4AaABAg   \n",
       "\n",
       "                                        CommentTitle   CommentCreationTime  \\\n",
       "0  \"It was a low degree of terror.\"\\nThis man cou...  2019-11-05T08:38:22Z   \n",
       "1  Could you imagine having to question if every ...  2019-11-04T13:16:13Z   \n",
       "2  That poor man never trusted another person and...  2019-11-30T14:30:31Z   \n",
       "3                 And this is only 1 person’s story.  2019-11-06T13:46:52Z   \n",
       "4  \"These interrogators, the investigators, they ...  2019-11-05T02:56:20Z   \n",
       "\n",
       "   CommentLikes  \n",
       "0         13004  \n",
       "1          7158  \n",
       "2          4516  \n",
       "3          2228  \n",
       "4          3026  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to DF\n",
    "vox_comments_df = pd.DataFrame(vox_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# vox_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1072df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vox_comments_df.to_csv(\"vox_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81617c4",
   "metadata": {},
   "source": [
    "#### MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_comments = get_video_comments('MSNBC', 'UUaXkIU1QidjPwiAYu6GcHjg', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f24f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msnbc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3abfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "msnbc_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3eded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "msnbc_comments_df = pd.DataFrame(msnbc_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# msnbc_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "msnbc_comments_df.to_csv(\"msnbc_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb41186",
   "metadata": {},
   "source": [
    "#### The Daily Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d666490",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 500 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAaflBUOkNOZFdJaEJEUkVVMFFUTTJNVVZETnpsQ056UkVLQUZJaUpHano4LTRoQU5RQVZvNElrTm9hRlpXVjBaWllUQnNWazFXUm5CYVIzQlJaREpzUWxkWVZUSlNNazVKWVcxalUwUkJhbk15WXkxMVFtaEVRWFJ3VTFSQlVTSQ&key=AIzaSyCfjrHtWz-ySCQMobOW0DrN3IwvIZL_YEE&alt=json returned \"Internal error encountered.\". Details: \"[{'domain': 'youtube.CoreErrorDomain', 'reason': 'SERVICE_UNAVAILABLE'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dailyshow_comments \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe Daily Show\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUwWhs_6x42TyRM4Wstoq8HA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 110\u001b[0m, in \u001b[0;36mget_video_comments\u001b[1;34m(channels, up_id, keyword_lists, limit)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m#for channel, upload_id in zip(channels, up_id):\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword_name, keywords \u001b[38;5;129;01min\u001b[39;00m keyword_lists\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 110\u001b[0m     videos_info \u001b[38;5;241m=\u001b[39m \u001b[43mkeyword_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUaXkIU1QidjPwiAYu6GcHjg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSNBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     video_comments \u001b[38;5;241m=\u001b[39m get_vid_comments(videos_info, limit)\n\u001b[0;32m    112\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(video_comments)\n",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m, in \u001b[0;36mget_video_comments.<locals>.keyword_videos\u001b[1;34m(playlist_id, keywords, channel_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Make the next API request using the nextPageToken\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mplaylistItems()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m     10\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         playlistId\u001b[38;5;241m=\u001b[39mplaylist_id,\n\u001b[0;32m     12\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[0;32m     13\u001b[0m     ) \n\u001b[1;32m---> 14\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Process the response and save video info\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\OSPython\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\OSPython\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 500 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAaflBUOkNOZFdJaEJEUkVVMFFUTTJNVVZETnpsQ056UkVLQUZJaUpHano4LTRoQU5RQVZvNElrTm9hRlpXVjBaWllUQnNWazFXUm5CYVIzQlJaREpzUWxkWVZUSlNNazVKWVcxalUwUkJhbk15WXkxMVFtaEVRWFJ3VTFSQlVTSQ&key=AIzaSyCfjrHtWz-ySCQMobOW0DrN3IwvIZL_YEE&alt=json returned \"Internal error encountered.\". Details: \"[{'domain': 'youtube.CoreErrorDomain', 'reason': 'SERVICE_UNAVAILABLE'}]\">"
     ]
    }
   ],
   "source": [
    "dailyshow_comments = get_video_comments('The Daily Show', 'UUwWhs_6x42TyRM4Wstoq8HA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40876ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dailyshow_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "dailyshow_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c92947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "dailyshow_comments_df = pd.DataFrame(dailyshow_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# dailyshow_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "dailyshow_comments_df.to_csv(\"dailyshow_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56a87d",
   "metadata": {},
   "source": [
    "#### Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21473d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments = get_video_comments('The Young Turks', 'UU1yBKRuGpC1tSM73A0ZjYjQ', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4395c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yturk_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "yturk_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "yturk_comments_df = pd.DataFrame(yturk_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# yturk_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "yturk_comments_df.to_csv(\"yturk_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c87bf",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dadcf5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\junk\\ipykernel_19132\\1937675395.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  right_comment_df = pd.read_csv('Project_yt_comments.csv')\n"
     ]
    }
   ],
   "source": [
    "right_comment_df = pd.read_csv('Project_yt_comments.csv')\n",
    "right_title_df = pd.read_csv('Project_yt_titles.csv')\n",
    "demo_df = pd.read_csv('combine_democ_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d0e2074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "        #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c4babbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df4fe8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN\n",
    "right_comment_df = right_comment_df.dropna()\n",
    "right_title_df = right_title_df.dropna()\n",
    "demo_df = demo_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f07fcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from datetime to date\n",
    "right_comment_df['CommentCreationTime'] = right_comment_df['CommentCreationTime'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "right_title_df['published_at'] = right_title_df['published_at'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "demo_df['CommentCreationTime'] = demo_df['CommentCreationTime'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())\n",
    "demo_df['published_at'] = demo_df['published_at'].apply(lambda x: datetime.strptime(str(x)[0:10], '%Y-%m-%d').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "301b1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "right_comment_df['TweetToken'] = right_comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "right_title_df['TweetToken'] = right_title_df['title'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "demo_df['TweetTokenTitle'] = demo_df['title'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "demo_df['TweetTokenComment'] = demo_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac148e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "right_comment_df['CommentCleaned'] = right_comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "right_title_df['TitleCleaned'] = right_title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "demo_df['TitleCleaned'] = demo_df['TweetTokenTitle'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "demo_df['CommentCleaned'] = demo_df['TweetTokenComment'].apply(lambda x: remove_stop(textcleaner(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6cdbfd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrc_sen(text, cat):\n",
    "    sen = NRCLex(text)\n",
    "    if cat == 'pos':\n",
    "        return sen.affect_frequencies['positive']\n",
    "    else:\n",
    "        return sen.affect_frequencies['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a374e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_comment_df['PositiveScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "right_comment_df['NegativeScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        \n",
    "right_title_df['PositiveScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "right_title_df['NegativeScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        \n",
    "\n",
    "demo_df['PositiveScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'pos'))\n",
    "demo_df['NegativeScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_sen(x, 'neg'))    \n",
    "demo_df['PositiveScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'pos'))    \n",
    "demo_df['NegativeScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_sen(x, 'neg'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d45f39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrc_emo(text, ver):\n",
    "    emo = NRCLex(text).affect_frequencies\n",
    "    max_emo = max(emo, key=emo.get)\n",
    "    max_score = emo[max_emo]\n",
    "    if ver == 'score':\n",
    "        return max_score\n",
    "    else:\n",
    "        return max_emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61c59024",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_comment_df['Emotion'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "right_comment_df['EmotionScore'] = right_comment_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'score'))        \n",
    "right_title_df['Emotion'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "right_title_df['EmotionScore'] = right_title_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'score'))        \n",
    "\n",
    "demo_df['EmotionTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "demo_df['EmotionScoreTitle'] = demo_df['TitleCleaned'].apply(lambda x: nrc_emo(x, 'score'))    \n",
    "demo_df['EmotionComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'emo'))\n",
    "demo_df['EmotionScoreComment'] = demo_df['CommentCleaned'].apply(lambda x: nrc_emo(x, 'score'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "      <th>TitleCleaned</th>\n",
       "      <th>...</th>\n",
       "      <th>PositiveScoreTitle</th>\n",
       "      <th>NegativeScoreTitle</th>\n",
       "      <th>PositiveScoreComment</th>\n",
       "      <th>NegativeScoreComment</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>EmotionScore</th>\n",
       "      <th>EmotionTitle</th>\n",
       "      <th>EmotionScoreTitle</th>\n",
       "      <th>EmotionComment</th>\n",
       "      <th>EmotionScoreComment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>Ugws1dFQrp7AovnexrB4AaABAg</td>\n",
       "      <td>Bless the hard work of journalists! Seeing the...</td>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>146</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgxcNLZW2rAeMBklWD14AaABAg</td>\n",
       "      <td>Also I can't imagine  the amount mental trauma...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>726</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgzFcqbEHILJ93hjvqh4AaABAg</td>\n",
       "      <td>This is so heartbreaking. What a horrific disp...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>251</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>UgxYrGf2RJZeKbFEap14AaABAg</td>\n",
       "      <td>7:35 Social-media shouldn't be just summarily ...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>219</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>Ugx5rmcME6ua2pMkojt4AaABAg</td>\n",
       "      <td>VICE NEVER Dissapoints! Amazing documentaries!...</td>\n",
       "      <td>2022-04-11</td>\n",
       "      <td>127</td>\n",
       "      <td>uncovered isis mass grave super users</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>BZ_f66aoZ0I</td>\n",
       "      <td>Kimberly Atkins Stohr: GA District Attorney Fa...</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>UgzBO4V2k1_ayMG_1tF4AaABAg</td>\n",
       "      <td>We don't trust bank s</td>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>1</td>\n",
       "      <td>kimberly atkins stohr ga district attorney fan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>trust</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>BZ_f66aoZ0I</td>\n",
       "      <td>Kimberly Atkins Stohr: GA District Attorney Fa...</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>UgxJiE3xA5crSIatswx4AaABAg</td>\n",
       "      <td>MSNBC at its worst. Fani Willis the new atm ca...</td>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>kimberly atkins stohr ga district attorney fan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>trust</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>BZ_f66aoZ0I</td>\n",
       "      <td>Kimberly Atkins Stohr: GA District Attorney Fa...</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>UgxXK_3Y17CdewRaIMJ4AaABAg</td>\n",
       "      <td>I bet Farni is not the only prosecutor in Ga. ...</td>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>9</td>\n",
       "      <td>kimberly atkins stohr ga district attorney fan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>BZ_f66aoZ0I</td>\n",
       "      <td>Kimberly Atkins Stohr: GA District Attorney Fa...</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Ugzd5fBC6pIR8k7JI214AaABAg</td>\n",
       "      <td>Exactly what defence can she use and according...</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>6</td>\n",
       "      <td>kimberly atkins stohr ga district attorney fan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>BZ_f66aoZ0I</td>\n",
       "      <td>Kimberly Atkins Stohr: GA District Attorney Fa...</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>UgyOIk-uTmv_QuWhlvd4AaABAg</td>\n",
       "      <td>We shouldn't have to fight ten times harder on...</td>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>0</td>\n",
       "      <td>kimberly atkins stohr ga district attorney fan...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2421 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     channel     video_id                                              title  \\\n",
       "0       Vice  SwoRx3tstxY      We Uncovered an ISIS Mass Grave | Super Users   \n",
       "1       Vice  SwoRx3tstxY      We Uncovered an ISIS Mass Grave | Super Users   \n",
       "2       Vice  SwoRx3tstxY      We Uncovered an ISIS Mass Grave | Super Users   \n",
       "3       Vice  SwoRx3tstxY      We Uncovered an ISIS Mass Grave | Super Users   \n",
       "4       Vice  SwoRx3tstxY      We Uncovered an ISIS Mass Grave | Super Users   \n",
       "...      ...          ...                                                ...   \n",
       "2416   MSNBC  BZ_f66aoZ0I  Kimberly Atkins Stohr: GA District Attorney Fa...   \n",
       "2417   MSNBC  BZ_f66aoZ0I  Kimberly Atkins Stohr: GA District Attorney Fa...   \n",
       "2418   MSNBC  BZ_f66aoZ0I  Kimberly Atkins Stohr: GA District Attorney Fa...   \n",
       "2419   MSNBC  BZ_f66aoZ0I  Kimberly Atkins Stohr: GA District Attorney Fa...   \n",
       "2420   MSNBC  BZ_f66aoZ0I  Kimberly Atkins Stohr: GA District Attorney Fa...   \n",
       "\n",
       "     keyword published_at                   CommentId  \\\n",
       "0       ISIS   2022-04-11  Ugws1dFQrp7AovnexrB4AaABAg   \n",
       "1       ISIS   2022-04-11  UgxcNLZW2rAeMBklWD14AaABAg   \n",
       "2       ISIS   2022-04-11  UgzFcqbEHILJ93hjvqh4AaABAg   \n",
       "3       ISIS   2022-04-11  UgxYrGf2RJZeKbFEap14AaABAg   \n",
       "4       ISIS   2022-04-11  Ugx5rmcME6ua2pMkojt4AaABAg   \n",
       "...      ...          ...                         ...   \n",
       "2416     Gas   2024-02-16  UgzBO4V2k1_ayMG_1tF4AaABAg   \n",
       "2417     Gas   2024-02-16  UgxJiE3xA5crSIatswx4AaABAg   \n",
       "2418     Gas   2024-02-16  UgxXK_3Y17CdewRaIMJ4AaABAg   \n",
       "2419     Gas   2024-02-16  Ugzd5fBC6pIR8k7JI214AaABAg   \n",
       "2420     Gas   2024-02-16  UgyOIk-uTmv_QuWhlvd4AaABAg   \n",
       "\n",
       "                                           CommentTitle CommentCreationTime  \\\n",
       "0     Bless the hard work of journalists! Seeing the...          2022-04-12   \n",
       "1     Also I can't imagine  the amount mental trauma...          2022-04-11   \n",
       "2     This is so heartbreaking. What a horrific disp...          2022-04-11   \n",
       "3     7:35 Social-media shouldn't be just summarily ...          2022-04-11   \n",
       "4     VICE NEVER Dissapoints! Amazing documentaries!...          2022-04-11   \n",
       "...                                                 ...                 ...   \n",
       "2416                              We don't trust bank s          2024-02-18   \n",
       "2417  MSNBC at its worst. Fani Willis the new atm ca...          2024-02-18   \n",
       "2418  I bet Farni is not the only prosecutor in Ga. ...          2024-02-17   \n",
       "2419  Exactly what defence can she use and according...          2024-02-16   \n",
       "2420  We shouldn't have to fight ten times harder on...          2024-02-17   \n",
       "\n",
       "      CommentLikes                                       TitleCleaned  ...  \\\n",
       "0              146              uncovered isis mass grave super users  ...   \n",
       "1              726              uncovered isis mass grave super users  ...   \n",
       "2              251              uncovered isis mass grave super users  ...   \n",
       "3              219              uncovered isis mass grave super users  ...   \n",
       "4              127              uncovered isis mass grave super users  ...   \n",
       "...            ...                                                ...  ...   \n",
       "2416             1  kimberly atkins stohr ga district attorney fan...  ...   \n",
       "2417             3  kimberly atkins stohr ga district attorney fan...  ...   \n",
       "2418             9  kimberly atkins stohr ga district attorney fan...  ...   \n",
       "2419             6  kimberly atkins stohr ga district attorney fan...  ...   \n",
       "2420             0  kimberly atkins stohr ga district attorney fan...  ...   \n",
       "\n",
       "     PositiveScoreTitle NegativeScoreTitle PositiveScoreComment  \\\n",
       "0              0.000000           0.333333             0.066667   \n",
       "1              0.000000           0.333333             0.230769   \n",
       "2              0.000000           0.333333             0.000000   \n",
       "3              0.000000           0.333333             0.166667   \n",
       "4              0.000000           0.333333             0.285714   \n",
       "...                 ...                ...                  ...   \n",
       "2416           0.333333           0.000000             0.000000   \n",
       "2417           0.333333           0.000000             0.142857   \n",
       "2418           0.333333           0.000000             0.000000   \n",
       "2419           0.333333           0.000000             0.074074   \n",
       "2420           0.333333           0.000000             0.000000   \n",
       "\n",
       "      NegativeScoreComment  Emotion  EmotionScore  EmotionTitle  \\\n",
       "0                 0.133333     fear      0.333333          fear   \n",
       "1                 0.076923     fear      0.333333          fear   \n",
       "2                 0.222222     fear      0.333333          fear   \n",
       "3                 0.166667     fear      0.333333          fear   \n",
       "4                 0.071429     fear      0.333333          fear   \n",
       "...                    ...      ...           ...           ...   \n",
       "2416              0.000000     fear      0.333333          fear   \n",
       "2417              0.000000     fear      0.333333          fear   \n",
       "2418              1.000000     fear      0.333333          fear   \n",
       "2419              0.111111     fear      0.333333          fear   \n",
       "2420              0.333333     fear      0.333333          fear   \n",
       "\n",
       "     EmotionScoreTitle  EmotionComment EmotionScoreComment  \n",
       "0             0.333333            fear            0.200000  \n",
       "1             0.333333        positive            0.230769  \n",
       "2             0.333333            fear            0.222222  \n",
       "3             0.333333            fear            0.166667  \n",
       "4             0.333333        positive            0.285714  \n",
       "...                ...             ...                 ...  \n",
       "2416          0.333333           trust            1.000000  \n",
       "2417          0.333333           trust            0.285714  \n",
       "2418          0.333333        negative            1.000000  \n",
       "2419          0.333333           anger            0.222222  \n",
       "2420          0.333333            fear            0.333333  \n",
       "\n",
       "[2421 rows x 23 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78024f9c",
   "metadata": {},
   "source": [
    "## Andy's Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id of a channel\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(part='contentDetails', forUsername=channel)\n",
    "    res = request.execute()\n",
    "    return res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "# Function for retrieving all vids within the upload playlist of a channel, stopping once a limit INT has been reached\n",
    "def get_vids(channel, limit, keywords, ideology):\n",
    "    \n",
    "    # Output list\n",
    "    vid_lst=[]\n",
    "\n",
    "    request = youtube.playlistItems().list(part='snippet',playlistId=get_upload_id(channel),maxResults=50)\n",
    "        \n",
    "    res = request.execute()\n",
    "    nextPageToken = res['nextPageToken']\n",
    "\n",
    "    # Iterate through each video in the playlist\n",
    "    for v in res[\"items\"]:\n",
    "\n",
    "        # Normalization of video title to check for keywords\n",
    "        title = v['snippet']['title']\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "        # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "        counter = 0\n",
    "        for word in title.split():\n",
    "            counter = 0\n",
    "            if word in keywords:\n",
    "                counter += 1\n",
    "        if counter == 0:\n",
    "            continue\n",
    "\n",
    "        # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "        vid_dict = {}\n",
    "        vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "        vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "        vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "        vid_dict['Ideology'] = ideology\n",
    "\n",
    "        # Separate Resource Call to retrieve video views\n",
    "        views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "        view_temp = views.execute()\n",
    "        vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "        # Append dictionary to greater list\n",
    "        vid_lst.append(vid_dict)\n",
    "\n",
    "    # Iterate until no more next page\n",
    "    while nextPageToken:\n",
    "        try:\n",
    "            request = youtube.playlistItems().list(part='snippet', playlistId=get_upload_id(channel), maxResults=50, pageToken = res['nextPageToken'])                \n",
    "            res = request.execute()\n",
    "\n",
    "            # Redefine next page token to check @ next iteration\n",
    "            nextPageToken = res['nextPageToken']\n",
    "\n",
    "            # Iterate through each video\n",
    "            for v in res[\"items\"]:\n",
    "\n",
    "                # Normalization of video title to check for keywords\n",
    "                title = v['snippet']['title']\n",
    "                title = title.lower()\n",
    "                title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "                # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "                counter = 0\n",
    "                for word in title.split():\n",
    "                    if word in keywords:\n",
    "                        counter += 1\n",
    "                if counter == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "                vid_dict = {}\n",
    "                vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "                vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "                vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "                                \n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "                \n",
    "                vid_lst.append(vid_dict)\n",
    "\n",
    "            # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "            if len(vid_lst) >= limit:\n",
    "                return(vid_lst)\n",
    "\n",
    "        # Error case handling\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "# Function for getting top 30 relevant comments for a list of videos\n",
    "def get_vid_comments(vid_lst, limit):\n",
    "    vids_final = []\n",
    "\n",
    "    # Iterate through each video in the video list\n",
    "    for vid in vid_lst:\n",
    "        \n",
    "        request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "        res = request.execute()\n",
    "\n",
    "        # Iterate through each comment\n",
    "        for v in res[\"items\"]:\n",
    "            \n",
    "            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "            vid_temp = copy.copy(vid)\n",
    "            vid_temp.update({'CommentId':v['id']})\n",
    "            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "            vids_final.append(vid_temp)\n",
    "\n",
    "        while nextPageToken:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "                res = request.execute()\n",
    "        \n",
    "                nextPageToken = res['nextPageToken']\n",
    "                \n",
    "                for v in res[\"items\"]:\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = copy.copy(vid)\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "                    \n",
    "                # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                if len(vids_final) >= limit:\n",
    "                    return(vids_final)\n",
    "            except KeyError:\n",
    "                break\n",
    "            \n",
    "    return vids_final\n",
    "\n",
    "# from Lab9\n",
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "        #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')\n",
    "\n",
    "def df_clean_process(df):\n",
    "\n",
    "    # Change datetime to date\n",
    "    df['VideoPublishedDate'] = df['VideoPublishedDate'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "    df['CommentCreationTime'] = df['CommentCreationTime'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "\n",
    "    # Check NaN, if < 10% of total dataset, drop NaN\n",
    "    if df.isnull().values.any():\n",
    "        if len(df[df.isna().any(axis=1)]) < len(df) * 0.1:\n",
    "            df = df.dropna()\n",
    "\n",
    "    # Split into separate df for computational load reduction\n",
    "    title_df = df[['ChannelName', 'VideoTitle', 'VideoPublishedDate', 'VideoViews', 'Ideology']].drop_duplicates()\n",
    "    comment_df = df[['ChannelName', 'VideoViews', 'CommentTitle', 'CommentCreationTime', 'CommentLikes', 'Ideology']]\n",
    "\n",
    "    # tokenize\n",
    "    title_df['TweetToken'] = title_df['VideoTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "    comment_df['TweetToken'] = comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "\n",
    "    # clean\n",
    "    title_df['Cleaned'] = title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "    comment_df['Cleaned'] = comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "\n",
    "    return (title_df, comment_df)\n",
    "\n",
    "    # Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c02602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define channels\n",
    "channels_left = ['VICE', 'Vox', 'MSNBC', 'The Daily Show', 'TheYoungTurks']\n",
    "channels_right = ['Fox News', 'Ben Shapiro', 'StevenCrowder', 'Daily Mail', 'DailyWire+']\n",
    "\n",
    "# define key ideologies/associated keywords to look for in title\n",
    "isis_keywords = ['terrorism', 'terrorist', 'extremism', 'radicalist', 'radicalism']\n",
    "guns_keywords = ['shooting', 'shootings', 'school shooting', 'school shootings', 'firearms', 'firearm', 'gun', 'gun control', 'guns', 'nra', 'second amendment']\n",
    "immigration_keywords = ['border control', 'mexico', 'visa', 'citizenship', 'asylum', 'deportation', 'refugee']\n",
    "economy_keywords = ['budget', 'budget deficit', 'unemployed', 'inflation', 'interest rate',' federal reserve', 'market', 'employment']\n",
    "health_care_keywords = ['medicaid', 'covid', 'obamacare', 'public health', 'insurance']\n",
    "socioeconomic_keywords = ['rich', 'poor', 'income inequality', 'poverty',' wealth distribution']\n",
    "abortion_keywords = ['pregnancy', 'unwanted pregnancy', 'roe', 'wade', 'abortion', 'pro-life', 'rape', 'incest', 'life of mother', 'religion']\n",
    "climate_change_keywords = ['global warming', 'carbon', 'alternative energy', 'climate', 'methane', 'emissions','gas','greenhouse']\n",
    "\n",
    "# Define for iteration\n",
    "keywords = [isis_keywords, guns_keywords, immigration_keywords, economy_keywords, health_care_keywords, socioeconomic_keywords, abortion_keywords, climate_change_keywords]\n",
    "\n",
    "# Pre-define empty df\n",
    "left_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "\n",
    "# Loop through all left channels\n",
    "for channel in channels_left:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        left_df = pd.concat([left_df,temp_df])\n",
    "\n",
    "# Pre-define empty df\n",
    "right_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)\n",
    "# Loop through all right channels\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
