{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ae97b8-7d0b-46d2-8e8d-6511fe658f50",
   "metadata": {},
   "source": [
    "# Emotional Consistency among Political Ideologies: An Approach to Address Polarization on Youtube\n",
    "\n",
    "Group 5:\n",
    "- Chance Landis (ChancL), Hanna Lee (Lee10), Jason Sun (YongXs), Andy Wong (WongA22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d5d02-a9ab-4a96-b211-909ac2a2e7ee",
   "metadata": {},
   "source": [
    "## Credit Listing\n",
    "- Hanna: Democratic-leaning Channels Data Collection\n",
    "- Chance: Republican-leaning Channels Data Collection\n",
    "- Andy: Exploratory Data Analysis\n",
    "- Jason: Text Documentation\n",
    "\n",
    "## Problem Statement\n",
    "We want to figure out whether the current political polarization is associated with the emotional values expressed by each party. Due to the current politically charged environment of our country, the semblance of sympathizing toward a value that is not related to your political party causes backlash. This fear of backlash can create a “false” polarized environment, which is an extension of the fear itself. The question that arises is whether these boundaries are reinforced by the people themselves and/or  external factors, like social media.\n",
    "\n",
    "## Research Question\n",
    "1. Do political parties exhibit similar emotional responses to differing ideologies?\n",
    "\n",
    "## Data Collection\n",
    "To investigate this topic, we will analyze content from the top five YouTube channels associated with Democratic and Republican viewpoints, based on subscriber counts. The channels selected for the study are:\n",
    "\n",
    "- **Democratic-leaning Channels**: Vice, Vox, MSNBC, The Daily Show, The Young Turks\n",
    "- **Republican-leaning Channels**: Fox News, Ben Shapiro, Steven Crowder, The Daily Mail, The Daily Wire\n",
    "\n",
    "We have identified eight key ideologies for this analysis to understand if there are emotional differences in how political parties discuss these topics. For each ideology, a set of keywords has been established to facilitate data scraping:\n",
    "a\n",
    "- **ISIS**: Terrorism, Extremism, Radical\n",
    "- **Guns**: Shootings, School shooting, Firearms, Gun control, NRA, Second Amendment\n",
    "- **Immigration**: Border control, Mexico, Visa /Citizenship, Asylum, Deportation, Refugee\n",
    "- **Economy**: Budget deficit, Unemployment, Inflation, Interest rate, Federal Reserve, Market, Employment\n",
    "- **Health care**: Medicaid, Covid, Obamacare, Public health, Insurance\n",
    "- **Socio-economic**: Rich / poor, Income inequality, Poverty, Wealth distribution\n",
    "- **Abortion**: Pregnancy, Unwanted Pregnancy, Roe, Wade, Abortion, Pro-life, Rape, Incest, Life of mother, Religion\n",
    "- **Climate change**: Global Warming, Carbo, Alternative Energy, Climate, Methane, Emissions, Gas, Greenhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199fa05",
   "metadata": {},
   "source": [
    "### Sources of Information\n",
    "- **AllSides**: A media bias tool that provides a rating based on \"multi-partisan Editorial Reviews by trained experts and Blind Bias Surveys™ in which participants rate content without knowing the source.\" We used this tool to determine how we should classify the most popular (based on subscriber count) YouTube channels we found. (Source: https://www.allsides.com/media-bias/media-bias-rating-methods)\n",
    "- **HypeAuitor**: A company that uses a data-driven approach to influencer marketing. In the process, they collated lists of YouTube based on category, subscriber count, and country. This allowed us to find YouTube channels that focused on news and politics with the most subscribers. (Source: https://hypeauditor.com/about/company/, https://hypeauditor.com/top-youtube-news-politics-united-states/)\n",
    "- **Pew Research Center**: A nonpartisan, nonprofit organization that conducts research on public opinion, demographic trends, and social issues. It provides data-driven insights into various aspects of social science issues, explicitly stating they do not take a stance on political issues. For our research, we relied on their studies on political ideologies and alignment with political parties as a reference. (Source: https://www.pewresearch.org/about/, https://www.pewresearch.org/politics/2016/06/22/5-views-of-parties-positions-on-issues-ideologies/)\n",
    "- **YouTube**: As a group, we've chosen to expand our collection of YouTube videos by selecting additional keywords associated with the ideology we're studying. Our focus will be on gathering comments from these videos to conduct our research.\n",
    "    - We used a combination of Andy and Hanna's code to get the comments from YouTube channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98165ca7",
   "metadata": {},
   "source": [
    "### Top 5 Democratic YouTube Channels\n",
    "Vice, Vox, MSNBC, The Daily Show, The Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d751659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.9/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7420f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client --quiet\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb56de50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c63afe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import googleapiclient\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e63ef0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API call\n",
    "# vice: AIzaSyA2rNi_MI-3LQkBzzQ6Tn4EF0lgXWoilfc\n",
    "# vox: AIzaSyAoeLCEEfqmnpRHR4xRMKt1YdbeUUw75ao\n",
    "# msnbc: AIzaSyBZDxP2HEW50EDfExcZJag7J2mRroZ9_vk\n",
    "# daily show: AIzaSyD8adQZlhLNVQrQXpU5-u3s1Y-9TZs20ik\n",
    "# young turk: AIzaSyB8yyrUrfQGLrlQRmF555oc1emrIDXF7yU\n",
    "\n",
    "# Others: API_KEY = \"AIzaSyCjWja_yyRROSw5tcP_KxYjasJgHLX3oKE\"\n",
    "# API_KEY = \"AIzaSyCjWja_yyRROSw5tcP_KxYjasJgHLX3oKE\"\n",
    "API_KEY = \"AIzaSyAuN1hrPKy3_h0_DC7aMEaRdeA9Ju_KzaE\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e06f5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels\n",
    "channels = [\"Vice\", \"Vox\", \"msnbc\", \"thedailyshow\", \"TheYoungTurks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95f6e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords\n",
    "keyword_lists = {\n",
    "    \"isis\": [\"ISIS\", \"Terrorism\", \"Radicalist\", \"Jihad\", \"Suicide Bombing\"],\n",
    "    \"guns\": [\"Gun\", \"Shooting\", \"School shooting\", \"Firearm\", \"Gun control\", \"NRA\", \"Second Amendment\"],\n",
    "    \"immigration\": [\"Immigration\", \"Border control\", \"Mexico\", \"Visa\", \"Citizenship\", \"Asylum\", \"Deportation\", \"Refugee\"],\n",
    "    \"economy\": [\"Economy\", \"Budget deficit\", \"Unemployed\", \"Inflation\", \"Interest rate\", \"Federal reserve\", \"Market\", \"Employment\"],\n",
    "    \"healthcare\": [\"Health care\", \"Medicaid\", \"Covid\", \"Obamacare\", \"Public health\", \"Insurance\"],\n",
    "    \"socioeco\": [\"Socio-economic\", \"Rich\", \"Poor\", \"Income inequality\", \"Poverty\", \"Wealth distribution\"],\n",
    "    \"abortion\": [\"Abortion\", \"Pregnancy\", \"Unwanted Pregnancy\", \"Roe\", \"Wade\", \"Pro-life\", \"Rape\", \"Incest\", \"Life of mother\", \"Religion\"],\n",
    "    \"climate\": [\"Climate change\", \"Global Warming\", \"Carbon\", \"Alternative Energy\", \"Climate\", \"Methane\", \"Emissions\", \"Gas\", \"Greenhouse\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51705702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting channel id based on name\n",
    "def get_channel_id(channel):  \n",
    "    channel_id = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        type=\"channel\",\n",
    "        q=channel\n",
    "    )\n",
    "\n",
    "    res_channel = channel_id.execute()\n",
    "    chan_id = res_channel[\"items\"][0][\"id\"][\"channelId\"]\n",
    "\n",
    "    return chan_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8307dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id using channel id\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel\n",
    "    )\n",
    "\n",
    "    res = request.execute()\n",
    "    uploads_playlist_id = res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "    return uploads_playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6401af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vice\n",
      "Vox\n",
      "msnbc\n",
      "thedailyshow\n",
      "TheYoungTurks\n"
     ]
    }
   ],
   "source": [
    "up_id = []\n",
    "\n",
    "for channel in channels:\n",
    "    print(channel)\n",
    "    chan_id = get_channel_id(channel)\n",
    "    upload_id = get_upload_id(chan_id)\n",
    "    up_id.append(upload_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "098f682b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UUn8zNIfYAQNdrFRrr8oibKw',\n",
       " 'UULXo7UDZvByw2ixzpQCufnA',\n",
       " 'UUaXkIU1QidjPwiAYu6GcHjg',\n",
       " 'UUwWhs_6x42TyRM4Wstoq8HA',\n",
       " 'UU1yBKRuGpC1tSM73A0ZjYjQ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54cfe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Function to check if a video title contains any of the keywords\n",
    "def contains_keyword(title, keywords):\n",
    "    title_lower = title.lower()\n",
    "    words = word_tokenize(title_lower)\n",
    "    \n",
    "    # Stem each word in the title + keyword\n",
    "    stemmed_words = [ps.stem(word) for word in words]\n",
    "    for keyword in keywords:\n",
    "        keyword_stemmed = ps.stem(keyword.lower())\n",
    "        if keyword_stemmed in stemmed_words:\n",
    "            return keyword\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "554d1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch videos from a playlist and get title with keywords\n",
    "def keyword_videos(playlist_id, keywords, channel_name):\n",
    "    videos_info = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Make the next API request using the nextPageToken\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=next_page_token\n",
    "        ) \n",
    "        res = request.execute()\n",
    "\n",
    "        # Process the response and save video info\n",
    "        for v in res[\"items\"]:\n",
    "            video_title = v[\"snippet\"][\"title\"]\n",
    "            detected_word = contains_keyword(video_title, keywords)\n",
    "            if detected_word:\n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                video_views = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "                # Append video information with views to videos_info list\n",
    "                videos_info.append({\n",
    "                    \"id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                    \"title\": video_title,\n",
    "                    \"keyword\": detected_word,\n",
    "                    \"published_at\": v[\"snippet\"][\"publishedAt\"],\n",
    "                    \"VideoViews\": video_views\n",
    "                })\n",
    "        # Update the nextPageToken for the next iteration\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token or (len(videos_info) > 30):\n",
    "            break\n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ae6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run the code for all the channels at once\n",
    "# This does not work because we run out of API calls\n",
    "\n",
    "# for channel, upload_id in zip(channels, up_id):\n",
    "#     for keyword_name, keywords in keyword_lists.items():\n",
    "#         videos_info = keyword_videos('UUn8zNIfYAQNdrFRrr8oibKw', keywords, 'Vice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9cb8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(channels, up_id, keyword_lists, limit=30):\n",
    "    # Function to fetch videos from a playlist and get title with keywordsand \n",
    "    def keyword_videos(playlist_id, keywords, channel_name):\n",
    "        videos_info = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            # Make the next API request using the nextPageToken\n",
    "            request = youtube.playlistItems().list(\n",
    "                part=\"snippet\",\n",
    "                playlistId=playlist_id,\n",
    "                pageToken=next_page_token\n",
    "            ) \n",
    "            res = request.execute()\n",
    "\n",
    "            # Process the response and save video info\n",
    "            for v in res[\"items\"]:\n",
    "                video_title = v[\"snippet\"][\"title\"]\n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    videos_info.append(\n",
    "                    {\n",
    "                        \"channel\": channel_name,\n",
    "                        \"video_id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"]\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "            # Update the nextPageToken for the next iteration\n",
    "            next_page_token = res.get('nextPageToken')\n",
    "\n",
    "            if not next_page_token or (len(videos_info) > 15):\n",
    "                break\n",
    "        return videos_info\n",
    "\n",
    "    # Function for getting top 30 relevant comments for a list of videos\n",
    "    def get_vid_comments(vid_lst, limit):\n",
    "        vids_final = []\n",
    "\n",
    "        # Iterate through each video in the video list\n",
    "        for vid in vid_lst:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    videoId=vid['video_id'],\n",
    "                    part='id,snippet,replies',\n",
    "                    textFormat='plainText',\n",
    "                    order='relevance',\n",
    "                    maxResults=50)\n",
    "                res = request.execute()\n",
    "\n",
    "                # Iterate through each comment\n",
    "                for v in res[\"items\"]:\n",
    "\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = vid.copy()\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "\n",
    "                nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                while nextPageToken:\n",
    "                    try:\n",
    "                        request = youtube.commentThreads().list(\n",
    "                            videoId=vid['video_id'],\n",
    "                            part='id,snippet,replies',\n",
    "                            textFormat='plainText',\n",
    "                            order='relevance',\n",
    "                            maxResults=50,\n",
    "                            pageToken=nextPageToken)\n",
    "\n",
    "                        res = request.execute()\n",
    "\n",
    "                        nextPageToken = res.get('nextPageToken')\n",
    "\n",
    "                        for v in res[\"items\"]:\n",
    "                            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                            vid_temp = vid.copy()\n",
    "                            vid_temp.update({'CommentId':v['id']})\n",
    "                            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                            vids_final.append(vid_temp)\n",
    "\n",
    "                        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                        if len(vids_final) >= limit:\n",
    "                            return vids_final\n",
    "                    except KeyError:\n",
    "                        break\n",
    "\n",
    "            # Error handling for videos with disabled comments\n",
    "            # Got the answer format from StackOverflow (https://stackoverflow.com/questions/19342111/get-http-error-code-from-requests-exceptions-httperror)\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(f\"Comments are disabled for the video with videoId: {vid['video_id']}\")\n",
    "                else:\n",
    "                    print(\"An HTTP error occurred:\", e)\n",
    "                # Continue to the next video\n",
    "                continue\n",
    "                \n",
    "        return vids_final\n",
    "    \n",
    "    all_comments = []\n",
    "    #for channel, upload_id in zip(channels, up_id):\n",
    "    for keyword_name, keywords in keyword_lists.items():\n",
    "        videos_info = keyword_videos('UUaXkIU1QidjPwiAYu6GcHjg', keywords, 'MSNBC')\n",
    "        video_comments = get_vid_comments(videos_info, limit)\n",
    "        all_comments.extend(video_comments)\n",
    "    \n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da646485",
   "metadata": {},
   "source": [
    "#### Vice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "801c67f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments are disabled for the video with videoId: EEIvWNhuL8U\n"
     ]
    }
   ],
   "source": [
    "vice_comments = get_video_comments('Vice', 'UUn8zNIfYAQNdrFRrr8oibKw', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0074edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe639e0d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugws1dFQrp7AovnexrB4AaABAg',\n",
       "  'CommentTitle': 'Bless the hard work of journalists! Seeing the deplorable and terrible things done by monstrous groups like ISIS in one spot must be so difficult. We’re with you!',\n",
       "  'CommentCreationTime': '2022-04-12T22:53:43Z',\n",
       "  'CommentLikes': 146},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxcNLZW2rAeMBklWD14AaABAg',\n",
       "  'CommentTitle': \"Also I can't imagine  the amount mental trauma this work puts these journalists and their teams  undergo having to file through hours of footage of some of the most horrific acts enacted upon people in order to try and piece together what really happened.  If they can uncover even some of truth that is a very big step forward to those victims who are still alive and hopefullt it can highlight those respsonsible\",\n",
       "  'CommentCreationTime': '2022-04-11T16:49:08Z',\n",
       "  'CommentLikes': 726},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzFcqbEHILJ93hjvqh4AaABAg',\n",
       "  'CommentTitle': 'This is so heartbreaking. What a horrific display of violence. I’m so sorry this happened to these people. I’m praying for their families 🥺🙏🏽',\n",
       "  'CommentCreationTime': '2022-04-11T15:08:45Z',\n",
       "  'CommentLikes': 251},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgxYrGf2RJZeKbFEap14AaABAg',\n",
       "  'CommentTitle': \"7:35 Social-media shouldn't be just summarily deleting flagged content when they remove stuff, they should be quarantining it, forwarding it to law-enforcement and making it available for investigators to requisition. We know they never delete any data anyway, so it's not like the media is actually _gone_ so it SHOULD still be available for the relevant authorities. Problem solved.\",\n",
       "  'CommentCreationTime': '2022-04-11T15:59:18Z',\n",
       "  'CommentLikes': 219},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugx5rmcME6ua2pMkojt4AaABAg',\n",
       "  'CommentTitle': 'VICE NEVER Dissapoints! Amazing documentaries! Thank you 🙏🏽 for your amazing humanity rights findings not only for one race or culture but for everyone god bless!! 🙌',\n",
       "  'CommentCreationTime': '2022-04-11T15:04:32Z',\n",
       "  'CommentLikes': 127},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgzzbQ-K80UX_H02FPR4AaABAg',\n",
       "  'CommentTitle': 'Surely if social media companies pull such content they need to forward onto investigators as part of their due diligence in terms of business-human rights.',\n",
       "  'CommentCreationTime': '2022-04-11T15:14:45Z',\n",
       "  'CommentLikes': 161},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'UgysAY65Ywm8Z54rrMZ4AaABAg',\n",
       "  'CommentTitle': 'Wow, this was interesting and I learned some things I didn’t know before. Thanks for “shedding light so others can see”, hey I like that! I appreciate good, honest journalism. Props. 👊',\n",
       "  'CommentCreationTime': '2022-04-11T18:23:50Z',\n",
       "  'CommentLikes': 33},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugwk0pK1S0kCyDg-QoZ4AaABAg',\n",
       "  'CommentTitle': 'Another great video by VICE, sometimes u guys produce mediocre content, but a lot of great stuff... Been following since the older documentary days about north korea',\n",
       "  'CommentCreationTime': '2022-04-12T05:37:50Z',\n",
       "  'CommentLikes': 93},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz8RjIShFG5SQ6XvyZ4AaABAg',\n",
       "  'CommentTitle': 'You guys are literally next level heros!!',\n",
       "  'CommentCreationTime': '2022-04-17T01:07:35Z',\n",
       "  'CommentLikes': 5},\n",
       " {'channel': 'Vice',\n",
       "  'video_id': 'SwoRx3tstxY',\n",
       "  'title': 'We Uncovered an ISIS Mass Grave | Super Users',\n",
       "  'keyword': 'ISIS',\n",
       "  'published_at': '2022-04-11T15:00:12Z',\n",
       "  'CommentId': 'Ugz1DoNZkAejBAok2Xd4AaABAg',\n",
       "  'CommentTitle': 'You guys r the most realistic, journalism i have seen. \\nThank you so much for bringing this',\n",
       "  'CommentCreationTime': '2022-04-11T16:59:17Z',\n",
       "  'CommentLikes': 4}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vice_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd175db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "vice_comments_df = pd.DataFrame(vice_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4760259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>Ugws1dFQrp7AovnexrB4AaABAg</td>\n",
       "      <td>Bless the hard work of journalists! Seeing the...</td>\n",
       "      <td>2022-04-12T22:53:43Z</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgxcNLZW2rAeMBklWD14AaABAg</td>\n",
       "      <td>Also I can't imagine  the amount mental trauma...</td>\n",
       "      <td>2022-04-11T16:49:08Z</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgzFcqbEHILJ93hjvqh4AaABAg</td>\n",
       "      <td>This is so heartbreaking. What a horrific disp...</td>\n",
       "      <td>2022-04-11T15:08:45Z</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>UgxYrGf2RJZeKbFEap14AaABAg</td>\n",
       "      <td>7:35 Social-media shouldn't be just summarily ...</td>\n",
       "      <td>2022-04-11T15:59:18Z</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice</td>\n",
       "      <td>SwoRx3tstxY</td>\n",
       "      <td>We Uncovered an ISIS Mass Grave | Super Users</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>2022-04-11T15:00:12Z</td>\n",
       "      <td>Ugx5rmcME6ua2pMkojt4AaABAg</td>\n",
       "      <td>VICE NEVER Dissapoints! Amazing documentaries!...</td>\n",
       "      <td>2022-04-11T15:04:32Z</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel     video_id                                          title keyword  \\\n",
       "0    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "1    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "2    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "3    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "4    Vice  SwoRx3tstxY  We Uncovered an ISIS Mass Grave | Super Users    ISIS   \n",
       "\n",
       "           published_at                   CommentId  \\\n",
       "0  2022-04-11T15:00:12Z  Ugws1dFQrp7AovnexrB4AaABAg   \n",
       "1  2022-04-11T15:00:12Z  UgxcNLZW2rAeMBklWD14AaABAg   \n",
       "2  2022-04-11T15:00:12Z  UgzFcqbEHILJ93hjvqh4AaABAg   \n",
       "3  2022-04-11T15:00:12Z  UgxYrGf2RJZeKbFEap14AaABAg   \n",
       "4  2022-04-11T15:00:12Z  Ugx5rmcME6ua2pMkojt4AaABAg   \n",
       "\n",
       "                                        CommentTitle   CommentCreationTime  \\\n",
       "0  Bless the hard work of journalists! Seeing the...  2022-04-12T22:53:43Z   \n",
       "1  Also I can't imagine  the amount mental trauma...  2022-04-11T16:49:08Z   \n",
       "2  This is so heartbreaking. What a horrific disp...  2022-04-11T15:08:45Z   \n",
       "3  7:35 Social-media shouldn't be just summarily ...  2022-04-11T15:59:18Z   \n",
       "4  VICE NEVER Dissapoints! Amazing documentaries!...  2022-04-11T15:04:32Z   \n",
       "\n",
       "   CommentLikes  \n",
       "0           146  \n",
       "1           726  \n",
       "2           251  \n",
       "3           219  \n",
       "4           127  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vice_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a39557ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vice_comments_df.to_csv(\"vice_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06c2de",
   "metadata": {},
   "source": [
    "### Vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5639ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_comments = get_video_comments('Vox', 'UULXo7UDZvByw2ixzpQCufnA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15dd8dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vox_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "100a954a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxCgDXiTTfjLBdkAE94AaABAg',\n",
       "  'CommentTitle': '\"It was a low degree of terror.\"\\nThis man could barely choke out those words a half century later...\\nLow degree?  I don\\'t think so.',\n",
       "  'CommentCreationTime': '2019-11-05T08:38:22Z',\n",
       "  'CommentLikes': 13004},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzpaYXLMtGz3ymErOJ4AaABAg',\n",
       "  'CommentTitle': 'Could you imagine having to question if every single person you engage with is a plant by some organization?',\n",
       "  'CommentCreationTime': '2019-11-04T13:16:13Z',\n",
       "  'CommentLikes': 7158},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgweQtEREv31yKgtumB4AaABAg',\n",
       "  'CommentTitle': 'That poor man never trusted another person and I feel so bad for him. I tear up every time he does. You can still see how hard he thinks before he speaks.',\n",
       "  'CommentCreationTime': '2019-11-30T14:30:31Z',\n",
       "  'CommentLikes': 4516},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwaArmK_X6VOrDrjrZ4AaABAg',\n",
       "  'CommentTitle': 'And this is only 1 person’s story.',\n",
       "  'CommentCreationTime': '2019-11-06T13:46:52Z',\n",
       "  'CommentLikes': 2228},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxzPYwa0EGXhdYhtMF4AaABAg',\n",
       "  'CommentTitle': '\"These interrogators, the investigators, they weren\\'t very bright people, but they had tremendous power over others. Over us.\" Some things absolutely never change',\n",
       "  'CommentCreationTime': '2019-11-05T02:56:20Z',\n",
       "  'CommentLikes': 3026},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgxL6re97jEJbZIqxBZ4AaABAg',\n",
       "  'CommentTitle': 'Boomer: it was so much better when I was younger',\n",
       "  'CommentCreationTime': '2019-11-05T17:15:07Z',\n",
       "  'CommentLikes': 3903},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'Ugwv4GDXYpk7qiHTInZ4AaABAg',\n",
       "  'CommentTitle': '\"They weren\\'t very bright people but they have tremendous power over others, over us.\" Sound familiar?',\n",
       "  'CommentCreationTime': '2019-11-04T23:31:59Z',\n",
       "  'CommentLikes': 3657},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgzDydG5LiAj1if6GvR4AaABAg',\n",
       "  'CommentTitle': 'I just want to offer a heartfelt thanks to the gentleman who was interviewed for this video. Sir, your courage and willingness to share about what was obviously a terrible time in your life (one that apparently affects you to this day) is invaluable to those of us who might otherwise have trouble connecting with that time and those events.',\n",
       "  'CommentCreationTime': '2019-11-04T14:09:00Z',\n",
       "  'CommentLikes': 11509},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgwwabWjgasZiMUny_p4AaABAg',\n",
       "  'CommentTitle': 'I want to give that man a hug after what he went through',\n",
       "  'CommentCreationTime': '2019-11-04T14:27:25Z',\n",
       "  'CommentLikes': 3315},\n",
       " {'channel': 'Vox',\n",
       "  'video_id': 'IbTBehjdlc0',\n",
       "  'title': 'How Florida legally terrorized gay students',\n",
       "  'keyword': 'Terrorism',\n",
       "  'published_at': '2019-11-04T13:00:06Z',\n",
       "  'CommentId': 'UgykYfLlDY9Gxiax96d4AaABAg',\n",
       "  'CommentTitle': 'Imagine hating someone because of their sexuality that has absolutely nothing to do with you.. lol',\n",
       "  'CommentCreationTime': '2020-07-10T15:23:49Z',\n",
       "  'CommentLikes': 2679}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "# vox_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40a023e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgxCgDXiTTfjLBdkAE94AaABAg</td>\n",
       "      <td>\"It was a low degree of terror.\"\\nThis man cou...</td>\n",
       "      <td>2019-11-05T08:38:22Z</td>\n",
       "      <td>13004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgzpaYXLMtGz3ymErOJ4AaABAg</td>\n",
       "      <td>Could you imagine having to question if every ...</td>\n",
       "      <td>2019-11-04T13:16:13Z</td>\n",
       "      <td>7158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgweQtEREv31yKgtumB4AaABAg</td>\n",
       "      <td>That poor man never trusted another person and...</td>\n",
       "      <td>2019-11-30T14:30:31Z</td>\n",
       "      <td>4516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgwaArmK_X6VOrDrjrZ4AaABAg</td>\n",
       "      <td>And this is only 1 person’s story.</td>\n",
       "      <td>2019-11-06T13:46:52Z</td>\n",
       "      <td>2228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox</td>\n",
       "      <td>IbTBehjdlc0</td>\n",
       "      <td>How Florida legally terrorized gay students</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2019-11-04T13:00:06Z</td>\n",
       "      <td>UgxzPYwa0EGXhdYhtMF4AaABAg</td>\n",
       "      <td>\"These interrogators, the investigators, they ...</td>\n",
       "      <td>2019-11-05T02:56:20Z</td>\n",
       "      <td>3026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  channel     video_id                                        title  \\\n",
       "0     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "1     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "2     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "3     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "4     Vox  IbTBehjdlc0  How Florida legally terrorized gay students   \n",
       "\n",
       "     keyword          published_at                   CommentId  \\\n",
       "0  Terrorism  2019-11-04T13:00:06Z  UgxCgDXiTTfjLBdkAE94AaABAg   \n",
       "1  Terrorism  2019-11-04T13:00:06Z  UgzpaYXLMtGz3ymErOJ4AaABAg   \n",
       "2  Terrorism  2019-11-04T13:00:06Z  UgweQtEREv31yKgtumB4AaABAg   \n",
       "3  Terrorism  2019-11-04T13:00:06Z  UgwaArmK_X6VOrDrjrZ4AaABAg   \n",
       "4  Terrorism  2019-11-04T13:00:06Z  UgxzPYwa0EGXhdYhtMF4AaABAg   \n",
       "\n",
       "                                        CommentTitle   CommentCreationTime  \\\n",
       "0  \"It was a low degree of terror.\"\\nThis man cou...  2019-11-05T08:38:22Z   \n",
       "1  Could you imagine having to question if every ...  2019-11-04T13:16:13Z   \n",
       "2  That poor man never trusted another person and...  2019-11-30T14:30:31Z   \n",
       "3                 And this is only 1 person’s story.  2019-11-06T13:46:52Z   \n",
       "4  \"These interrogators, the investigators, they ...  2019-11-05T02:56:20Z   \n",
       "\n",
       "   CommentLikes  \n",
       "0         13004  \n",
       "1          7158  \n",
       "2          4516  \n",
       "3          2228  \n",
       "4          3026  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change to DF\n",
    "vox_comments_df = pd.DataFrame(vox_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# vox_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04e03a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "vox_comments_df.to_csv(\"vox_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a7b5e",
   "metadata": {},
   "source": [
    "#### MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7535f617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 500 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAaflBUOkNQRkFJaEEyTVRVMU56TXdPRVU1UkRGQk9FTTNLQUZJaUpHano4LTRoQU5RQVZvNElrTm9hRlpXVjBaWllUQnNWazFXUm5CYVIzQlJaREpzUWxkWVZUSlNNazVKWVcxalUwUkJhbk15WXkxMVFtaEVRWFJ3VTFSQlVTSQ&key=AIzaSyAuN1hrPKy3_h0_DC7aMEaRdeA9Ju_KzaE&alt=json returned \"Internal error encountered.\". Details: \"[{'message': 'Internal error encountered.', 'domain': 'global', 'reason': 'backendError'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m msnbc_comments \u001b[38;5;241m=\u001b[39m \u001b[43mget_video_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSNBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUaXkIU1QidjPwiAYu6GcHjg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mget_video_comments\u001b[0;34m(channels, up_id, keyword_lists, limit)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#for channel, upload_id in zip(channels, up_id):\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword_name, keywords \u001b[38;5;129;01min\u001b[39;00m keyword_lists\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 110\u001b[0m     videos_info \u001b[38;5;241m=\u001b[39m \u001b[43mkeyword_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUUaXkIU1QidjPwiAYu6GcHjg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMSNBC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     video_comments \u001b[38;5;241m=\u001b[39m get_vid_comments(videos_info, limit)\n\u001b[1;32m    112\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(video_comments)\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mget_video_comments.<locals>.keyword_videos\u001b[0;34m(playlist_id, keywords, channel_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Make the next API request using the nextPageToken\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mplaylistItems()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     10\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m         playlistId\u001b[38;5;241m=\u001b[39mplaylist_id,\n\u001b[1;32m     12\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[1;32m     13\u001b[0m     ) \n\u001b[0;32m---> 14\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Process the response and save video info\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 500 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUaXkIU1QidjPwiAYu6GcHjg&pageToken=EAAaflBUOkNQRkFJaEEyTVRVMU56TXdPRVU1UkRGQk9FTTNLQUZJaUpHano4LTRoQU5RQVZvNElrTm9hRlpXVjBaWllUQnNWazFXUm5CYVIzQlJaREpzUWxkWVZUSlNNazVKWVcxalUwUkJhbk15WXkxMVFtaEVRWFJ3VTFSQlVTSQ&key=AIzaSyAuN1hrPKy3_h0_DC7aMEaRdeA9Ju_KzaE&alt=json returned \"Internal error encountered.\". Details: \"[{'message': 'Internal error encountered.', 'domain': 'global', 'reason': 'backendError'}]\">"
     ]
    }
   ],
   "source": [
    "msnbc_comments = get_video_comments('MSNBC', 'UUaXkIU1QidjPwiAYu6GcHjg', keyword_lists, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d516e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msnbc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c68463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "msnbc_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "msnbc_comments_df = pd.DataFrame(msnbc_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# msnbc_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "msnbc_comments_df.to_csv(\"msnbc_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9fb86",
   "metadata": {},
   "source": [
    "#### The Daily Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyshow_comments = get_video_comments('The Daily Show', 'UUwWhs_6x42TyRM4Wstoq8HA', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dailyshow_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "dailyshow_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab77f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "dailyshow_comments_df = pd.DataFrame(dailyshow_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# dailyshow_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dc2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "dailyshow_comments_df.to_csv(\"dailyshow_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf4362",
   "metadata": {},
   "source": [
    "#### Young Turks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments = get_video_comments('The Young Turks', 'UU1yBKRuGpC1tSM73A0ZjYjQ', keyword_lists, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d941304",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yturk_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb7132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output, commented out for viewing purposes\n",
    "yturk_comments[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to DF\n",
    "yturk_comments_df = pd.DataFrame(yturk_comments)\n",
    "\n",
    "# Check output, commented out for viewing purposes\n",
    "# yturk_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6074ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "yturk_comments_df.to_csv(\"yturk_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c46357",
   "metadata": {},
   "source": [
    "#### Combine all DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3270ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yturk_comments_df = pd.read_csv('yturk_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9321a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_democ_comments = pd.concat([vice_comments_df, vox_comments_df, yturk_comments_df])\n",
    "\n",
    "combine_democ_comments.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27f092bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2421, 9)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_democ_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e7cb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "combine_democ_comments.to_csv(\"combine_democ_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e7b31",
   "metadata": {},
   "source": [
    "### Top 5 Republican YouTube Channels\n",
    "Fox News, Ben Shapiro, Steven Crowder, The Daily Mail, The Daily Wire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eaa6286-0613-475f-b7da-d02f68797328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch videos from a playlist and get title with keywordsand \n",
    "def keyword_videos_right(playlist_id, channel_name, dict_list):\n",
    "    videos_info = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        # Make the next API request using the nextPageToken\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet\",\n",
    "            playlistId=playlist_id,\n",
    "            pageToken=next_page_token\n",
    "        ) \n",
    "        res = request.execute()\n",
    "\n",
    "        # Process the response and save video info\n",
    "        for v in res[\"items\"]:\n",
    "            video_title = v[\"snippet\"][\"title\"]\n",
    "            for keyword_name, keywords in keyword_lists.items():\n",
    "            \n",
    "                detected_word = contains_keyword(video_title, keywords)\n",
    "                if detected_word:\n",
    "                    # Separate Resource Call to retrieve video views\n",
    "                    views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                    view_temp = views.execute()\n",
    "                    video_views = view_temp['items'][0]['statistics']['viewCount']\n",
    "    \n",
    "                    # Append video information with views to videos_info list\n",
    "                    dict_list.append({\n",
    "                        \"id\": v[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"channel_name\" : v['snippet']['channelTitle'],\n",
    "                        \"title\": video_title,\n",
    "                        \"keyword\": detected_word,\n",
    "                        \"published_at\": v[\"snippet\"][\"publishedAt\"],\n",
    "                        \"VideoViews\": video_views\n",
    "                    })\n",
    "        # Update the nextPageToken for the next iteration\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if not next_page_token or (len(videos_info) > 60):\n",
    "            break\n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37911dc2-44d7-4718-a5c9-c16847e330bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channels\n",
    "channels_right = [\"BenShapiro\", \"StevenCrowder\", \"FoxNews\", \"DailyWirePlus\", \"dailymail\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065935f2-fd8f-4837-8ca4-745e26c6ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets list of Right channels playlist id for uploads\n",
    "right_up_id = []\n",
    "for channel in channels_right:\n",
    "    chan_id = get_channel_id(channel)\n",
    "    upload_id = get_upload_id(chan_id)\n",
    "    right_up_id.append(upload_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c298dbb2-a6e7-427a-a3f0-d5513a54e790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UUnQC_G5Xsjhp9fEJKuIcrSw',\n",
       " 'UUIveFvW-ARp_B_RckhweNJw',\n",
       " 'UUXIJgqnII2ZOINSWNOGFThA',\n",
       " 'UUaeO5vkdj5xOQHp4UmIN6dw',\n",
       " 'UUw3fku0sH3qA3c3pZeJwdAw']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_up_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1512a786-b5c5-47f6-9571-a8ba10e2c45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIS', 'Terrorism', 'Extremism', 'Radicalist']\n",
      "['Gun', 'Shooting', 'School shooting', 'Firearm', 'Gun control', 'NRA', 'Second Amendment']\n",
      "['Immigration', 'Border control', 'Mexico', 'Visa', 'Citizenship', 'Asylum', 'Deportation', 'Refugee']\n",
      "['Economy', 'Budget deficit', 'Unemployed', 'Inflation', 'Interest rate', 'Federal reserve', 'Market', 'Employment']\n",
      "['Health care', 'Medicaid', 'Covid', 'Obamacare', 'Public health', 'Insurance']\n",
      "['Socio-economic', 'Rich', 'Poor', 'Income inequality', 'Poverty', 'Wealth distribution']\n",
      "['Abortion', 'Pregnancy', 'Unwanted Pregnancy', 'Roe', 'Wade', 'Pro-life', 'Rape', 'Incest', 'Life of mother', 'Religion']\n",
      "['Climate change', 'Global Warming', 'Carbon', 'Alternative Energy', 'Climate', 'Methane', 'Emissions', 'Gas', 'Greenhouse']\n"
     ]
    }
   ],
   "source": [
    "for keyword_name, keywords in keyword_lists.items():\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7696c-e66d-46cb-b0c6-c3a760282a98",
   "metadata": {},
   "source": [
    "## Code for collecting titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832180d0-1323-460d-9542-eb78d44e89d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BenShapiro\n",
      "StevenCrowder\n",
      "FoxNews\n",
      "DailyWirePlus\n",
      "dailymail\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUw3fku0sH3qA3c3pZeJwdAw&pageToken=EAAaflBUOkNOa1dJaEJGTWpjMk1ETXhOams1TWtKQk9ETTFLQUZJNk9qXzRlNjNoQU5RQVZvNElrTm9hRlpXV0dONldtMTBNVTFJVGtsTk0wWkNUVEpOZW1OR2NHeFRibVJyVVZoalUwUkJhbVJxY3paMVFtaEVRWEZOVjJkQlVTSQ&key=AIzaSyCmEsJx2xNF4_-5c7KtY1zeIftuVsD0UHc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel, upload_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(channels_right, right_up_id):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(channel)\n\u001b[0;32m----> 5\u001b[0m     videos_info \u001b[38;5;241m=\u001b[39m \u001b[43mkeyword_videos_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupload_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_video_titles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#right_video_titles.append(videos_info)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mkeyword_videos_right\u001b[0;34m(playlist_id, channel_name, dict_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Make the next API request using the nextPageToken\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mplaylistItems()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m      9\u001b[0m         part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         playlistId\u001b[38;5;241m=\u001b[39mplaylist_id,\n\u001b[1;32m     11\u001b[0m         pageToken\u001b[38;5;241m=\u001b[39mnext_page_token\n\u001b[1;32m     12\u001b[0m     ) \n\u001b[0;32m---> 13\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Process the response and save video info\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=snippet&playlistId=UUw3fku0sH3qA3c3pZeJwdAw&pageToken=EAAaflBUOkNOa1dJaEJGTWpjMk1ETXhOams1TWtKQk9ETTFLQUZJNk9qXzRlNjNoQU5RQVZvNElrTm9hRlpXV0dONldtMTBNVTFJVGtsTk0wWkNUVEpOZW1OR2NHeFRibVJyVVZoalUwUkJhbVJxY3paMVFtaEVRWEZOVjJkQlVTSQ&key=AIzaSyCmEsJx2xNF4_-5c7KtY1zeIftuVsD0UHc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "# Collects video titles for each of the given channels that contain keywords given\n",
    "right_video_titles = []\n",
    "for channel, upload_id in zip(channels_right, right_up_id):\n",
    "    print(channel)\n",
    "    videos_info = keyword_videos_right(upload_id, channel, right_video_titles)\n",
    "    #right_video_titles.append(videos_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9bd8f10-d727-498b-a7bd-07e8c7da3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_df = pd.DataFrame(right_video_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc381da7-6206-4898-a8e0-86af469262c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1997, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0f00669-d8af-424e-994b-62a89c7e5d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>VideoViews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cF24uzD9EMI</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Another Shooting at a Church in Houston</td>\n",
       "      <td>Shooting</td>\n",
       "      <td>2024-02-13T22:30:04Z</td>\n",
       "      <td>162443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VOkkGuOqQVY</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Illegal Immigrants Pummel Cops, Walk Free</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>2024-02-01T18:00:11Z</td>\n",
       "      <td>190787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p0lr2GE_tYc</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Pro-Life Protesters Arrested</td>\n",
       "      <td>Pro-life</td>\n",
       "      <td>2024-01-31T22:30:07Z</td>\n",
       "      <td>67120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re1nbhsUCE4</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>The \"Magic Word\" for Immigrants</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>2024-01-31T20:00:21Z</td>\n",
       "      <td>107104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m0An8qb5jSs</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Biden's Immigration Policy</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>2024-01-31T00:30:32Z</td>\n",
       "      <td>259543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>K6FVKaG5jco</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Old Joe Isn’t Afraid To Nuke All You Gun Nuts ...</td>\n",
       "      <td>Gun</td>\n",
       "      <td>2021-06-24T17:00:09Z</td>\n",
       "      <td>298474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>nS4Qv0YC6kU</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>INFLATION: How Money-Printing &amp; President Bide...</td>\n",
       "      <td>Inflation</td>\n",
       "      <td>2021-06-19T16:38:20Z</td>\n",
       "      <td>425234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>oEAzF_-rzuo</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Biden Domestic Terror Plan Will Include ‘Anti-...</td>\n",
       "      <td>Terrorism</td>\n",
       "      <td>2021-06-16T22:44:11Z</td>\n",
       "      <td>122365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>kOaF8wMnVus</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Ben Shapiro Explains the Beauty of the Free Ma...</td>\n",
       "      <td>Market</td>\n",
       "      <td>2021-06-06T21:00:12Z</td>\n",
       "      <td>155202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>RPbII0BhvzE</td>\n",
       "      <td>Ben Shapiro</td>\n",
       "      <td>Shapiro Reveals The 3 Keys To Escaping Generat...</td>\n",
       "      <td>Poverty</td>\n",
       "      <td>2021-06-06T13:00:08Z</td>\n",
       "      <td>267900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id channel_name  \\\n",
       "0    cF24uzD9EMI  Ben Shapiro   \n",
       "1    VOkkGuOqQVY  Ben Shapiro   \n",
       "2    p0lr2GE_tYc  Ben Shapiro   \n",
       "3    re1nbhsUCE4  Ben Shapiro   \n",
       "4    m0An8qb5jSs  Ben Shapiro   \n",
       "..           ...          ...   \n",
       "195  K6FVKaG5jco  Ben Shapiro   \n",
       "196  nS4Qv0YC6kU  Ben Shapiro   \n",
       "197  oEAzF_-rzuo  Ben Shapiro   \n",
       "198  kOaF8wMnVus  Ben Shapiro   \n",
       "199  RPbII0BhvzE  Ben Shapiro   \n",
       "\n",
       "                                                 title      keyword  \\\n",
       "0              Another Shooting at a Church in Houston     Shooting   \n",
       "1            Illegal Immigrants Pummel Cops, Walk Free  Immigration   \n",
       "2                         Pro-Life Protesters Arrested     Pro-life   \n",
       "3                      The \"Magic Word\" for Immigrants  Immigration   \n",
       "4                           Biden's Immigration Policy  Immigration   \n",
       "..                                                 ...          ...   \n",
       "195  Old Joe Isn’t Afraid To Nuke All You Gun Nuts ...          Gun   \n",
       "196  INFLATION: How Money-Printing & President Bide...    Inflation   \n",
       "197  Biden Domestic Terror Plan Will Include ‘Anti-...    Terrorism   \n",
       "198  Ben Shapiro Explains the Beauty of the Free Ma...       Market   \n",
       "199  Shapiro Reveals The 3 Keys To Escaping Generat...      Poverty   \n",
       "\n",
       "             published_at VideoViews  \n",
       "0    2024-02-13T22:30:04Z     162443  \n",
       "1    2024-02-01T18:00:11Z     190787  \n",
       "2    2024-01-31T22:30:07Z      67120  \n",
       "3    2024-01-31T20:00:21Z     107104  \n",
       "4    2024-01-31T00:30:32Z     259543  \n",
       "..                    ...        ...  \n",
       "195  2021-06-24T17:00:09Z     298474  \n",
       "196  2021-06-19T16:38:20Z     425234  \n",
       "197  2021-06-16T22:44:11Z     122365  \n",
       "198  2021-06-06T21:00:12Z     155202  \n",
       "199  2021-06-06T13:00:08Z     267900  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecb71cf4-8293-4d12-8884-e980dfb4c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV\n",
    "right_df.to_csv('Project_yt_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b695249-6aa4-446d-8564-dbca93f14d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'VOkkGuOqQVY',\n",
       " 'channel_name': 'Ben Shapiro',\n",
       " 'title': 'Illegal Immigrants Pummel Cops, Walk Free',\n",
       " 'keyword': 'Immigration',\n",
       " 'published_at': '2024-02-01T18:00:11Z',\n",
       " 'VideoViews': '190787'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_video_titles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad0986-b82b-44ae-95f2-afd1487a53b9",
   "metadata": {},
   "source": [
    "## Code to get Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5786cf1-dd08-4b1b-88d2-ef4c09336d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting comments for a given of videos\n",
    "def get_vid_comments_right(vid, limit):\n",
    "    vids_final = []\n",
    "    \n",
    "    # Iterate through each video in the video list\n",
    "    request = youtube.commentThreads().list(\n",
    "        videoId=vid['id'],\n",
    "        part='id,snippet,replies',\n",
    "        textFormat='plainText',\n",
    "        order='relevance',\n",
    "        maxResults=100\n",
    "    )\n",
    "    res = request.execute()\n",
    "\n",
    "    # Iterate through each comment\n",
    "    try:\n",
    "        while res[\"nextPageToken\"] != None:\n",
    "            for v in res[\"items\"]:\n",
    "                # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                vid_temp = vid.copy()\n",
    "                vid_temp.update({'CommentId':v['id']})\n",
    "                vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                vids_final.append(vid_temp)\n",
    "            \n",
    "            request = youtube.commentThreads().list(\n",
    "                videoId=vid['id'],\n",
    "                part='id,snippet,replies',\n",
    "                textFormat='plainText',\n",
    "                order='relevance',\n",
    "                maxResults=100,\n",
    "                pageToken = res[\"nextPageToken\"]\n",
    "            )\n",
    "            res = request.execute()\n",
    "    except KeyError:\n",
    "        for v in res[\"items\"]:\n",
    "                # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                vid_temp = vid.copy()\n",
    "                vid_temp.update({'CommentId':v['id']})\n",
    "                vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                vids_final.append(vid_temp)\n",
    "        # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "        if len(vids_final) >= limit:\n",
    "            return(vids_final)\n",
    "            \n",
    "            \n",
    "    return vids_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c40208a-5b88-4556-811a-d66bf97e0be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?videoId=XOAyAUhuzLk&part=id%2Csnippet%2Creplies&textFormat=plainText&order=relevance&maxResults=100&key=AIzaSyD9fIFqDX7zzn8RP3mj1typ9zXxJECtujg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m right_comments_dict_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m right_video_titles:\n\u001b[0;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_vid_comments_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     right_comments_dict_list\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m, in \u001b[0;36mget_vid_comments_right\u001b[0;34m(vid, limit)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through each video in the video list\u001b[39;00m\n\u001b[1;32m      6\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39mcommentThreads()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m      7\u001b[0m     videoId\u001b[38;5;241m=\u001b[39mvid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid,snippet,replies\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Iterate through each comment\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?videoId=XOAyAUhuzLk&part=id%2Csnippet%2Creplies&textFormat=plainText&order=relevance&maxResults=100&key=AIzaSyD9fIFqDX7zzn8RP3mj1typ9zXxJECtujg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">"
     ]
    }
   ],
   "source": [
    "# Runs code to get comments for each title in right_video_titles\n",
    "right_comments_dict_list = []\n",
    "for title in right_video_titles:\n",
    "    result = get_vid_comments_right(title, 100)\n",
    "    right_comments_dict_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5e46d20-7846-4434-bfd5-923ca82ccdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(right_comments_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "076c6f4a-45f8-48ab-a43d-1632727daa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts list of list of dictionaries to a flat list\n",
    "flat_list_of_dicts = [item for sublist in right_comments_dict_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4dc3923-7cdd-4608-9d83-ca0eca5cb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts list of dictionaries to dataframe\n",
    "right_comments_df = pd.DataFrame(flat_list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "441f70d2-2312-4f43-905e-600fb6675f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>title</th>\n",
       "      <th>keyword</th>\n",
       "      <th>published_at</th>\n",
       "      <th>VideoViews</th>\n",
       "      <th>CommentId</th>\n",
       "      <th>CommentTitle</th>\n",
       "      <th>CommentCreationTime</th>\n",
       "      <th>CommentLikes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>282010</th>\n",
       "      <td>l-4VF0ZqHCM</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Heroes stand out in Kansas City parade shootin...</td>\n",
       "      <td>Shooting</td>\n",
       "      <td>2024-02-15T18:08:42Z</td>\n",
       "      <td>11383</td>\n",
       "      <td>UgyO-SdUx2guZzRw5o14AaABAg</td>\n",
       "      <td>And we are defending Ukraine's Border?</td>\n",
       "      <td>2024-02-15T22:25:29Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282011</th>\n",
       "      <td>l-4VF0ZqHCM</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Heroes stand out in Kansas City parade shootin...</td>\n",
       "      <td>Shooting</td>\n",
       "      <td>2024-02-15T18:08:42Z</td>\n",
       "      <td>11383</td>\n",
       "      <td>UgxSjwiKL59ivVjYHOV4AaABAg</td>\n",
       "      <td>PS Trump stinks</td>\n",
       "      <td>2024-02-15T19:14:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id channel_name  \\\n",
       "282010  l-4VF0ZqHCM     Fox News   \n",
       "282011  l-4VF0ZqHCM     Fox News   \n",
       "\n",
       "                                                    title   keyword  \\\n",
       "282010  Heroes stand out in Kansas City parade shootin...  Shooting   \n",
       "282011  Heroes stand out in Kansas City parade shootin...  Shooting   \n",
       "\n",
       "                published_at VideoViews                   CommentId  \\\n",
       "282010  2024-02-15T18:08:42Z      11383  UgyO-SdUx2guZzRw5o14AaABAg   \n",
       "282011  2024-02-15T18:08:42Z      11383  UgxSjwiKL59ivVjYHOV4AaABAg   \n",
       "\n",
       "                                  CommentTitle   CommentCreationTime  \\\n",
       "282010  And we are defending Ukraine's Border?  2024-02-15T22:25:29Z   \n",
       "282011                         PS Trump stinks  2024-02-15T19:14:02Z   \n",
       "\n",
       "        CommentLikes  \n",
       "282010             0  \n",
       "282011             1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_comments_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d616dd3-1ffb-4d7e-ab92-c5ac74d3b21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282012, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_comments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa18b17c-2eaa-433b-be70-aa9f624a6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF as CSV\n",
    "right_comments_df.to_csv('Project_yt_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67867d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab0d9947",
   "metadata": {},
   "source": [
    "## Andy's Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f57878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for retrieving the upload playlist id of a channel\n",
    "def get_upload_id(channel):\n",
    "    request = youtube.channels().list(part='contentDetails', forUsername=channel)\n",
    "    res = request.execute()\n",
    "    return res[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "# Function for retrieving all vids within the upload playlist of a channel, stopping once a limit INT has been reached\n",
    "def get_vids(channel, limit, keywords, ideology):\n",
    "    \n",
    "    # Output list\n",
    "    vid_lst=[]\n",
    "\n",
    "    request = youtube.playlistItems().list(part='snippet',playlistId=get_upload_id(channel),maxResults=50)\n",
    "        \n",
    "    res = request.execute()\n",
    "    nextPageToken = res['nextPageToken']\n",
    "\n",
    "    # Iterate through each video in the playlist\n",
    "    for v in res[\"items\"]:\n",
    "\n",
    "        # Normalization of video title to check for keywords\n",
    "        title = v['snippet']['title']\n",
    "        title = title.lower()\n",
    "        title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "        # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "        counter = 0\n",
    "        for word in title.split():\n",
    "            counter = 0\n",
    "            if word in keywords:\n",
    "                counter += 1\n",
    "        if counter == 0:\n",
    "            continue\n",
    "\n",
    "        # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "        vid_dict = {}\n",
    "        vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "        vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "        vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "        vid_dict['Ideology'] = ideology\n",
    "\n",
    "        # Separate Resource Call to retrieve video views\n",
    "        views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "        view_temp = views.execute()\n",
    "        vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "\n",
    "        # Append dictionary to greater list\n",
    "        vid_lst.append(vid_dict)\n",
    "\n",
    "    # Iterate until no more next page\n",
    "    while nextPageToken:\n",
    "        try:\n",
    "            request = youtube.playlistItems().list(part='snippet', playlistId=get_upload_id(channel), maxResults=50, pageToken = res['nextPageToken'])                \n",
    "            res = request.execute()\n",
    "\n",
    "            # Redefine next page token to check @ next iteration\n",
    "            nextPageToken = res['nextPageToken']\n",
    "\n",
    "            # Iterate through each video\n",
    "            for v in res[\"items\"]:\n",
    "\n",
    "                # Normalization of video title to check for keywords\n",
    "                title = v['snippet']['title']\n",
    "                title = title.lower()\n",
    "                title = re.sub(r'[^\\w\\s]','', title)\n",
    "\n",
    "                # Check for key words. If key word detected, then counter +1. If counter > 0, then the post will be flagged and added.\n",
    "                counter = 0\n",
    "                for word in title.split():\n",
    "                    if word in keywords:\n",
    "                        counter += 1\n",
    "                if counter == 0:\n",
    "                    continue\n",
    "\n",
    "                # Create temp dictionary per video, and add video-specific information to dictionary\n",
    "                vid_dict = {}\n",
    "                vid_dict['ChannelName'] = v['snippet']['channelTitle']\n",
    "                vid_dict['VideoId'] = v['snippet']['resourceId']['videoId']\n",
    "                vid_dict['VideoTitle'] = v['snippet']['title']\n",
    "                                \n",
    "                # Separate Resource Call to retrieve video views\n",
    "                views = youtube.videos().list(id=v['snippet']['resourceId']['videoId'], part=\"snippet,contentDetails,statistics\")\n",
    "                view_temp = views.execute()\n",
    "                vid_dict['VideoViews'] = view_temp['items'][0]['statistics']['viewCount']\n",
    "                \n",
    "                vid_lst.append(vid_dict)\n",
    "\n",
    "            # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "            if len(vid_lst) >= limit:\n",
    "                return(vid_lst)\n",
    "\n",
    "        # Error case handling\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "# Function for getting top 30 relevant comments for a list of videos\n",
    "def get_vid_comments(vid_lst, limit):\n",
    "    vids_final = []\n",
    "\n",
    "    # Iterate through each video in the video list\n",
    "    for vid in vid_lst:\n",
    "        \n",
    "        request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "        res = request.execute()\n",
    "\n",
    "        # Iterate through each comment\n",
    "        for v in res[\"items\"]:\n",
    "            \n",
    "            # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "            vid_temp = copy.copy(vid)\n",
    "            vid_temp.update({'CommentId':v['id']})\n",
    "            vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "            vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "            vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "            vids_final.append(vid_temp)\n",
    "\n",
    "        while nextPageToken:\n",
    "            try:\n",
    "                request = youtube.commentThreads().list(videoId=vid['VideoId'],part='id,snippet,replies',textFormat='plainText',order='relevance',maxResults=50)\n",
    "                res = request.execute()\n",
    "        \n",
    "                nextPageToken = res['nextPageToken']\n",
    "                \n",
    "                for v in res[\"items\"]:\n",
    "                    # Create a copy of dictionary of current video that is being iterated. This is because each comment is also contained with the video data\n",
    "                    vid_temp = copy.copy(vid)\n",
    "                    vid_temp.update({'CommentId':v['id']})\n",
    "                    vid_temp.update({'CommentTitle':v['snippet']['topLevelComment']['snippet']['textOriginal']})\n",
    "                    vid_temp.update({'CommentCreationTime':v['snippet']['topLevelComment']['snippet']['publishedAt']})\n",
    "                    vid_temp.update({'CommentLikes':v['snippet']['topLevelComment']['snippet']['likeCount']})\n",
    "                    vids_final.append(vid_temp)\n",
    "                    \n",
    "                # If the number of saved videos is larger than self-defined limit, break while loop and return the list of videos\n",
    "                if len(vids_final) >= limit:\n",
    "                    return(vids_final)\n",
    "            except KeyError:\n",
    "                break\n",
    "            \n",
    "    return vids_final\n",
    "\n",
    "# from Lab9\n",
    "def textcleaner(row):\n",
    "    row = str(row)\n",
    "    row = row.lower()\n",
    "    # remove punctuation\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "        #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row\n",
    "    \n",
    "stopeng = set(stopwords.words('english'))\n",
    "def remove_stop(text):\n",
    "    try:\n",
    "        words = text.split(' ')\n",
    "        valid = [x for x in words if x not in stopeng]\n",
    "        return(' '.join(valid))\n",
    "    except AttributeError:\n",
    "        return('')\n",
    "\n",
    "def df_clean_process(df):\n",
    "\n",
    "    # Change datetime to date\n",
    "    df['VideoPublishedDate'] = df['VideoPublishedDate'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "    df['CommentCreationTime'] = df['CommentCreationTime'].apply(lambda x: datetime.strptime(x[0:10], '%Y-%m-%d').date())\n",
    "\n",
    "    # Check NaN, if < 10% of total dataset, drop NaN\n",
    "    if df.isnull().values.any():\n",
    "        if len(df[df.isna().any(axis=1)]) < len(df) * 0.1:\n",
    "            df = df.dropna()\n",
    "\n",
    "    # Split into separate df for computational load reduction\n",
    "    title_df = df[['ChannelName', 'VideoTitle', 'VideoPublishedDate', 'VideoViews', 'Ideology']].drop_duplicates()\n",
    "    comment_df = df[['ChannelName', 'VideoViews', 'CommentTitle', 'CommentCreationTime', 'CommentLikes', 'Ideology']]\n",
    "\n",
    "    # tokenize\n",
    "    title_df['TweetToken'] = title_df['VideoTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "    comment_df['TweetToken'] = comment_df['CommentTitle'].apply(lambda x: casual.TweetTokenizer().tokenize(x))\n",
    "\n",
    "    # clean\n",
    "    title_df['Cleaned'] = title_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "    comment_df['Cleaned'] = comment_df['TweetToken'].apply(lambda x: remove_stop(textcleaner(x)))\n",
    "\n",
    "    return (title_df, comment_df)\n",
    "\n",
    "    # Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3501557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define channels\n",
    "channels_left = ['VICE', 'Vox', 'MSNBC', 'The Daily Show', 'TheYoungTurks']\n",
    "channels_right = ['Fox News', 'Ben Shapiro', 'StevenCrowder', 'Daily Mail', 'DailyWire+']\n",
    "\n",
    "# define key ideologies/associated keywords to look for in title\n",
    "isis_keywords = ['terrorism', 'terrorist', 'extremism', 'radicalist', 'radicalism']\n",
    "guns_keywords = ['shooting', 'shootings', 'school shooting', 'school shootings', 'firearms', 'firearm', 'gun', 'gun control', 'guns', 'nra', 'second amendment']\n",
    "immigration_keywords = ['border control', 'mexico', 'visa', 'citizenship', 'asylum', 'deportation', 'refugee']\n",
    "economy_keywords = ['budget', 'budget deficit', 'unemployed', 'inflation', 'interest rate',' federal reserve', 'market', 'employment']\n",
    "health_care_keywords = ['medicaid', 'covid', 'obamacare', 'public health', 'insurance']\n",
    "socioeconomic_keywords = ['rich', 'poor', 'income inequality', 'poverty',' wealth distribution']\n",
    "abortion_keywords = ['pregnancy', 'unwanted pregnancy', 'roe', 'wade', 'abortion', 'pro-life', 'rape', 'incest', 'life of mother', 'religion']\n",
    "climate_change_keywords = ['global warming', 'carbon', 'alternative energy', 'climate', 'methane', 'emissions','gas','greenhouse']\n",
    "\n",
    "# Define for iteration\n",
    "keywords = [isis_keywords, guns_keywords, immigration_keywords, economy_keywords, health_care_keywords, socioeconomic_keywords, abortion_keywords, climate_change_keywords]\n",
    "\n",
    "# Pre-define empty df\n",
    "left_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "\n",
    "# Loop through all left channels\n",
    "for channel in channels_left:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        left_df = pd.concat([left_df,temp_df])\n",
    "\n",
    "# Pre-define empty df\n",
    "right_df = pd.DataFrame(columns=['ChannelName', 'VideoId', 'VideoTitle', 'Ideology', 'VideoPublishedDate', 'VideoViews', 'CommentId', 'CommentTitle', 'CommentCreationTime', 'CommentLikes'])\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)\n",
    "# Loop through all right channels\n",
    "for channel in channels_right:\n",
    "\n",
    "    # Loop through all keywords/ideologies\n",
    "    for keyword, ideology in zip(keywords, ['ISIS', 'GUNS', 'IMMIGRATION', 'ECONOMY', 'HEALTH CARE', 'SOCIOECONOMIC', 'ABORTION', 'CLIMATE CHANGE']):\n",
    "\n",
    "        # Return temp df for one ideology for one channel\n",
    "        temp_df = pd.DataFrame(get_vid_comments(get_vids(channel, 50, keyword, ideology)[0:50], 150))\n",
    "\n",
    "        # Append temp df to master df\n",
    "        right_df = pd.concat([right_df,temp_df])\n",
    "\n",
    "(left_title_df, left_comment_df) = df_clean_process(left_df)\n",
    "(right_title_df, right_comment_df) = df_clean_process(right_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
